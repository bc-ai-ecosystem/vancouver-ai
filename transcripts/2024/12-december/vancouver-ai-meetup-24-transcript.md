# Vancouver AI Community Meetup #24 - December 2024 Transcript
## NeurIPS Special Edition & KK Birthday Celebration

**Date:** December 2024
**Location:** Space Center, Vancouver, BC
**Event:** Vancouver AI Community Meetup #24 - NeurIPS Special Edition
**Host:** [[Kris Krüg]] (@kriskrug)
**Special Occasion:** [[Kris Krüg]]'s Birthday Celebration

---

There's a lot of people. Check, check, one, two, three. Hey everybody, how's it going out there? Yeah, welcome to the December Vancouver AI Community Meetup. You are the Vancouver AI community.  Yeah,  joined by a whole bunch of people from NeurIPS, which I'm really excited about. I went to a bunch of Vancouver events this week that were in conjunction with NeurIPS, and I didn't see any NeurIPS at any of the Vancouver events.

So I'm so excited that the demos you're seeing out there in the lobby with Metacreation Lab, uh, Metacreation Lab, and some of the speakers right here tonight are coming directly over from NURB. So I'm really proud of that. And it's really cool that we get to cross pollinate our Vancouver community with the big international machine learning community that's going on out there.

Check check one two, check check one two.  It's my birthday. I hate birthdays. I hate birthdays. I had to tell everybody that because it was today, because it was going on. December birthdays suck. Your parents try to give you Christmas gifts and birthday gifts at the same time.  Also, like, in elementary school, remember you gave, like, invites to everyone in your class, but a lot of the time, it was like, mine was, like, already into the Christmas break, and so no one would ever come.

Awww.  So, this is, like, growth and progress from, you know, Santa being like, yo, it's our birthday!  So I'm reaching you guys all showing up. I know it's busy right now with NeurIPS. There's like five competing events tonight. It's Christmas time. But I'm really glad you're here because I've pulled together all the most interesting things that I know about and I want to show them to you here tonight, so.

And um, it's a bit of an all star cast. You guys saw Orbit Princess, the Holy Mother of the Void, dancing there in the lobby. She's been with us since the beginning and she's been experimenting with movement and sensors and AI and human embodiment as a Counterbalance to all this kind of digital projection that we're talking about and stuff.

Um, tonight's, uh, event is special again in some ways because we're doing it in partnership with the Meta Creation Lab at SFU, Felipe, and he's going to talk here tonight and help organize a lot of the demos in the lobby. We're also doing this in conjunction with Enya Learning, which is one of the coolest nonprofits that I've learned about in the last six months.

They've got some AI assistants that help immigrant students kind of carry with them all through their learning here in Canada. They stick with them, they build a second brain of the knowledge, they help tutor them, and, uh, augment the traditional learning experience in a really cool way. And so Amin from Enya and I were able to put our heads and wallets together and bring out something really special from Montreal, an AI called Vot, that you'll get to interact with later, I'm very excited to show you that.

Um, without further ado, I'm going to invite Patrick, the Enya father up here, Patrick. Patrick is a professor at the Emerging Media Lab at UBC, and a big contributor to this community. He asked you guys last month, uh, he did some crowd prompting, and you gave him some prompts, and he wrote a song, and he's here tonight to share the song with us.

Patrick. so much.

Woo! I'll give you some of these. Ow! Okay, enough of that. How is everybody?

Awesome.  I don't know what the piano's doing here, but there's a mic, so I'll speak into it, and maybe something will come out of the piano. Um, but what I want to do is announce that today, uh, along with colleague, uh, James Rout, James, wave. Where are you, James? Hello.  Uh, we are, uh, giving away the first chapter of the Krampus and Santa songbook,  which was the winner from three weeks ago.

And it was not entirely made with AI. And that is what the important part is to say. That was part of the act.

So, all that to say, there's a free chapter waiting for everyone as a reward. But to, uh, receive the reward, I need your help in contributions in the creation of the book. The song, and this is one of the songs in the songbook. To do that, I'm going to divide you into two parts. Uh, the part, just kind of like right in front of me, down.

That'll be one part, to the right. And then the other part, to my left.  It's, yeah, perfect, so. And it's not those who can't sing on the right, those who can on the left. Just, just,  um. But it's really easy. It's, it, you just have to memorize these words. I had to, so, now you have to. So.  And it's a really easy tune.

That's it. That's the tune.  Uh, a nice little vamp. And your lines, here on the left, if I could hear, is  Closing your stocking.  your turn.  Colding your stocking. Yeah. That's nice. That's a nice voice. One more time. I wanna hear that. Closing your stocking. Oh yeah.  What more? I like saving your stocking. You're do now.

I believe you can do better.

Your lines are gifts. Oh, gifts under the tree.  You don't actually have to do the melody at all.  I'm gonna point to you.  It'll be like, one, two, three, and Coals in your stocking, Gifts under the tree. One more time. Coals in your stocking, Gifts under the tree. One more time. Coals in your stocking, Gifts under the tree.

Cool, and then I need a little rhythm track, so you're gonna have to snap at the same time. And here we go, one more time. Coals in your stocking, Put some sparkling wines and beers in that bucket back there if you get thirsty.  Alright, now, we're going to examine who's been naughty, and who's been naughtier, as we've all had to reconcile a year of surviving AI.

Yeah,  you knew those two letters were coming, and here they are.  Okay, so, by a show of hands, let's do a little bit of research.  This is, uh, don't worry, I went through ethics on this. By a show of hands, how many of you have increased your use of AI in the past year?

Whoa, okay, and by another show of hands,  how many of you have read the end user license agreements for all the private machine or the public machine learning models that you've been using?

Okay,  by another show of hands, how many of you have used AI even though it's been banned by your organization?  I see some hands up! Awesome!  And one last one, I know that, okay, now be honest here, don't worry, nobody is recording. Oof! Yeah, it's research, and I know about ethics. Okay, so, here's the last one.

How many of you  Have used AI entirely to write an email.

That's not bad. Let's sing the chorus again. And!  Cause in your stocking.  With a little string. One more time. Cause in your stocking.  Gifts under the tree.  Bowls in your stocking. Gifts under the tree. Okay, now, this is where it gets a little testier, so I don't know if we should do, like, put your hands up.  On this one, I'll leave it to you.

How many of you, by a show of hands,  Okay, but, actually, keep your hands forward.  How many of you have been involved in the use of generating any AI system to do with surveilling human movement? very much.  And the hands went up, and here we go! Two, three, and sing it! Cold fingers talking, kiss under a tree.

Cold fingers talking, kiss under a tree. And here's the last one. This is the hardest one. And please be honest, or don't be. By a show of non hands and folded arms in front of your chest, how many of you have been involved in the development of AI systems that are meant to increase the sentience of AI in order for it to eventually weaponize against humanity.

One, two, three, let's go! Closing the cycle,  it's under the tree. You know, that's too, it's too opposite opted. I'm going to give you guys a chance here. Gifts in your stocking. One, two, three, and Gifts in your stocking. Yeah, that's right. Gifts in the stocking. Coal under the tree. Coal under the tree. Yeah, okay, this is a competition now.

One, two, three, and Gifts in your stocking. Coal under the tree. One more time. Gifts in your stocking. Coal under the tree. Alright, now my fabulous stochastic parents are going to Repeat after me. Do do do do do Do diddly do Do diddly do Diddly diddly diddly do Diddly diddly do Do diddly do do do do Diddly diddly do Diddly diddly did did Diddly diddly did WOAH!

Yeah!  Cool! Gifts  and COAL

you're kind. It's a clap.  Before Chris comes up though, I just want to remind you of your reward. And I also, uh, want to say that come and see me, uh, for any type of collaboration and to receive your e book, uh, chapter. And, uh, Chris is now turning, I don't know, like 71 today, so let's applaud that. Let's applaud that.

And, uh,  speaking of rewards, there is a reward coming, and I need all of you to join me in that familiar, uh, patterned birthday song. Yeah, we're gonna do it better because now you've had a warm up for your voices.  We are gonna sing until you squirm, my friend.  Here we go, everyone. You know the tune. Do you want it?

Do you want a note?  Two, three.  Happy birthday to you.  Happy birthday to you. Awesome. Happy  birthday to.

Happy birthday to you. Even harmonies!

Happy birthday, Chris. This was supposed to have 10, 000 candles on it, but the Space Center said no. Um, I love this event so much. Thank you, Patrick, for coming and bringing your creative gifts. Patrick has brought so much to this community over the year that we've been doing this. Uh, one time he made a pot of stew and served it to everybody.

Um, he's done a bunch of songs, he's done some call and response, and he's made a bunch of great content here. He's included people from this community in the books that he's been working on and the research papers he's been working on. In fact, he's used this community as a case study for AI adoption and education globally.

Um, and I just really appreciate your involvement, Patrick. Thanks to you guys for letting me put all the weird things I find interesting up here. Can I hear it for Patrick Penfather?

Hey Pixel Wizard,  come hang out for a sec. Oh gosh.  Um, this is [[Kevin Friel]], Mr. Pixel Wizard, lots of you guys know him. Uh,  he's like my main man, I trust him completely with lots of different things and helps me make all this amazing shit happen. Um, some of the things you saw back there, they look a little pedestrian, but they're really actually super fancy.

I didn't know I was supposed to talk. This was, uh, this is how Chris rolls. Actually, it's a little sick. When we first moved into the Space Center, When we first moved into the Space Center, Um,  We had to change a few things from when this used to be in my studio. You remember when it used to be in my studio?

Everyone was kind of gathered around. As I saw your faces, I'd call you up. I'd ask what was on your mind. And we got moved into a little bit more formal environment. But a lot of you have been asking for feedback. [[Matthew Schwartzman]] and some other people were like, we liked it when everyone could come talk and you surprised us sometimes and stuff.

So I'm going to try to do a mix of the formal awesome speeches that we're, you know, accustomed to that where we all learn something. And putting Pixel Wizard on the spot to talk about his stable diffusion, generative AI backgrounds that he's making over there on NVIDIA 4090.  Well, that's about it. It's a, uh, stable diffusion platform running out 49 now.

you've been a great audience. Try the fish. Um,  uh, so yeah, I mean, that's basically it. It, this is a, a, a combination of, of touch designer that's running kind of the video io platform. It's running out of a, uh, black magic, uh, video interface, which gives us a stable output on it. Um, no pun on Stable Diffusion, but they're, they're in the same universe right now, so.

Uh, this is running Stable Diffusion Turbo right now on an NVIDIA 4090. It's running at 14 frames per second. I'm capping that, actually. It can run at 30. I kinda like how it Why did you build this? I built this because I used to be in News Graphics, working for Global, CTV, and the Canadian political channel CPAC, and Hey, we got a CPAC fan out here.

You must have enjoyed our work on the Senate committee hearings.  Those were my favorite.  Not. Uh, Stable cash though. Um,  The, the, the development of like coming up with graphics in a real time environment like that, which I still hold as like an incredible way to, um, Tell stories and learn how to tell stories.

You're generating a lot of things beforehand. So we have an election coming up and we have to know everything about every person in every writing, which is basically a thousand people. And we have to have the storytelling components of that built well in advance. It would take us a month to do that. And there are always surprises that would happen.

And we would have to scurry around in real time to build something as quick as possible. Okay. And what better way to add production value in environments where things are changing constantly all the time than to have something that's actually real time responsive. And you give it a prompt and it gives it, gives you some, some backing visuals for it.

And that was the impetus for it. Um, also getting into like the podcast stage with, uh, with our, you know, cinematic, uh, podcast platform. Just adding some production value to keep some background graphics in context as subject matter comes up because conversations are fluid. And I wanted those graphics to be as fluid.

So that's kind of the backing story of what we've got going on here. Plus it's really fun to put these things up in, in these kinds of scenarios. So thank you, bud. It's been a while. Like when me and Kevin are doing podcasts, I got the Amazon events. Are we doing at a NeurIPS event this week? I'm sitting there interviewing people I've never met.

And the graphics behind us are changing based on the conversation he's generating locally. It's pretty amazing, actually. So, thanks for being a part of this. I just wanted a chance to show off your cool stuff. Oh, thanks man. Appreciate it. Happy birthday. Thank you, brother. Mr. Christian.  Um,  You all know I'm pretty sensitive about, like, the commercial aspect of these things and trying to keep it away.

But I will give a plug for Kevin and say, if you need someone to come set up a, like, high quality, Hollywood level production environment, I'm For a day with his lights and his generative backgrounds and all this real time So he can make hundreds of thousands of dollars worth of Hollywood content for 10, 000 a day or something like that So it's like it's pretty fucking amazing shit, really, you know Like those of you who are spending money like that on these types of things, you'll know what I'm talking about  Okay up next I am proud to introduce you to [[Carol Anne Hilton]] Carolyn Hilton, I get to work with and for at the Indigenomics Institute and the Global Center for Indigenomics.

Um, she's a world leader when it comes to indigenous economic reconciliation, and she's got some really far out ideas. They sound, um, audacious at times, and then they come true. Um, she's got her second book coming out soon, and we're working together on a whole bunch of experiments. Um, related to indigenous AI and the economic reconciliation movement that she is a part of.

So Caroline, you want to come just say a few things about what's on your mind?

And, uh, for being just so you know, I'm going to have you say one or two things about move 37 when she's done.  Thank you for being born, Chris. Woo.

Good class is [[Carol Anne Hilton]] has great conversation, class walker, to escort to empty. I am new channel from the West coast of Vancouver Island. I wanted to acknowledge we are on Squamish territory and also bring attention to this moment. What a absolute powerful time to be alive. We actually are this generation that gets to be alive right now, which is like, You In any time, in any across space, across time, we get to be here right now and that's a beautiful, powerful thing.

And the conversations that we have of how we focus and what our inheritance of experience of humanity has been.  I am of the belief that AI is a tool to be able to further activate our humanity. And the best of that, in terms of my heritage as a New Toronto person, our language model is literally tens of thousands of years old.

And in it, it holds the  code of success of what it means to be human, what it means to be aligned in the world, what it means to be showing up in this time, to be able to understand what it is that we've inherited, and what it is that essentially we are responsible for in our future. Our language holds all of that knowledge system.

That is then further transmitted into these teachings that tell us about what our responsibility of how we show up in this time and what the, in this moment.  That is not only what it means to be Indigenous, that is also what it means to be human. And that is what I mean of what a powerful time to be alive.

Indigenomics. So I did this thing, I created this like movement and I went to school for business. I did not go to school on how to run a global movement but here I am running a global movement. It's pretty awesome. One of my really, really big questions is  The process of activating Indigenous economic power.

And I use that very specifically, and as we think of Indigenous people, it's not often necessarily in terms of economy, and nor is it necessarily in terms of power. But to realize that, essentially, the construct of this moment, and while I have framed that largely in a Canadian context, um, Um, the idea of being able to utilize AI as a tool, if you told me this time last year that I would have, um, three core AI advisors on the team, that we would have built an indigenomics AI, um, engine, that indigenomics AI as an, um, economic intelligence  I would have said you are crazy, but here we are and all of that is activated and happening.

And I think it's really important to  realize the acceleration of what is possible. And being able to focus on that question of what is possible and the experience of what is possible means that we need to be able to unpack how we've shown up here. We're a lot of what Um, our experiences is what has happened to us.

And it is in this moment that we get to experience design bias.  And that is what is in this power moment.  I recently did a, um, longer term strategic planning session with my team of advisors. And to realize, to reverse engineer from the outcome of a sustainable global model that advances the structured outcomes and investability of an indigenous world economic forum.

That's what we're building with Indigenomics AI. And I've been so blessed to have Chris come onto our team, where our core activities to look at shaping the narrative of a movement is primarily how Chris and I initially engaged. I'm like, how do you do that? And to be able to have this time to be able to experiment with that, to be able to shape that using and building AI tools, and to realize that the advancement of core structures, the space of specifically identifying  indigeneity, indigenous language, indigenous worldview,  ethical space, and.

Centering this experience of the responsibility of life itself. That to me is the advancement of indigenous AI and this space of indigenomics AI, the constructive build in terms of the, our ability to present into Canadian reality and global reality. A capital markets division that lifts and moves beyond an experience of the current limitations of  sustainable finance.

And to realize that the tools and structures, the advancement and alignment of that, specifically with technology, that's everything, and that is this moment. We're all building in these places of this beautiful moment of what is possible. Let's have the courage to do this together. Thank you.

It's my birthday, so I gotta be sincere, even if I'm gonna cry. Uh, Caroline is one of the people I respect the most in this whole wide world, and I'm so lucky that she saw in me what she was looking for, and I get to contribute my gifts to the beautiful, amazing shit that she's doing. Um, it is  Meaningful to me that I can play this role as an ally out there exploring the edges of all this AI stuff and bringing The best of what I find back to you and for you to remix all those ideas with your vision  This is very important work.

Thanks for the opportunity. Yeah, [[Carol Anne Hilton]] Perveen from move 37 is about to come up and myself from future proof creatives and [[Lorraine Lowe]] from the space center All right Are working on a collaboration for next April 8th, 9th, and 10th. I'll let them talk to you more about that another time. But just know that there's a indigenous AI technology event.

Come on up, Raveen. Um, with AI Meetup Vibes happening here at the Space Center next April 8th, 9th, and 10th.  I've known this guy for like 20 years.  He's got a very, very diverse background. Um, he's worked with Doctors Without Borders. He's, uh, done, uh, High level global thought leadership, um, in academia and beyond.

Um, but he started a really cool collective of artists and technologists called move 37 XR.  One of his partners in that is John Desnoyers Stewart, who has a permanent exhibit upstairs on the second floor called Star Stuff. And later, when we break for networking again, you're allowed to wander through the whole space center.

And I encourage you to find out, find Star Stuff. Go hang out there, it's really great. Praveen, you got two minutes to talk about whatever you want, bro. Sure. Okay. So, Praveen Pillay from Move37XR, and I'm in an audience, or with an audience that understands Move37. Yes? No. No? Okay. So, Move37. Uh, the reason for that name came from the, uh, the, uh, when AlphaGo played the game Go against Lee Sedol.

Move 37 was the first non human move in  2, 000 years of play that innovated, it just thought in a different way.  And given that we are in a time of incredible change. Um, so the, uh, one of the areas, the hats that I wear is as a community fellow at the Center for Global Health Research.  And we've hit some really, shall we say, interesting places in planetary boundaries.

So you heard Chris mention that I was with Doctrine of Borders, helped to establish them here in Canada. And on my board, along with Carol Ann, I have James Urbinsky, who was accepted the Nobel Peace Prize for our organization. Thank you very much. And so, about 25 years ago, I was saying to James, you know, um, about 2015, this is, it's really going to get bad.

There's going to be some things that are going to intersect, and hopefully we won't be needed, but maybe we need to get moving on this. And this is where we started talking about some of the innovations that would be required to navigate this type of transition.  So those planetary boundaries are being exceeded.

And one of the most important things, and so where this is comes to move 37 is the idea of, you know, up until now we've been very human. We've been navigating in a way that's actually quite predictable for thousands of years, and we're now hit those planetary boundaries of that trajectory. So how do we get past that?

So move 37, is there a way that there can be a symbiotic relationship  with a non human consciousness or intelligence? Let's frame it that way for now.  And can that, um, I love Patrick's, um, shall we say humorous song,  right? The call and the gift, and that's what we're looking at.  And so how do we work with this non human intelligence to deepen and extend so that we actually can inhabit that regenerative future that we can, rather than a 300 year time horizon, let's look at a 10, 000 year time horizon for this species to continue our journey.

And that's the potential, right? So Move 37 XR comes from the immersive realities, right? XR.  And so the reason I got involved with that was that my youngest son, and I've been involved with human computer interaction, machine learning,  surveillance, since the late 90s, and backed out of all of that because of the military implications.

And, uh, but now it's come to a place where my youngest son asked me for a set of, uh, an Oculus a couple years ago, three years ago.  And I was working on a project, this project at MIT with the Presencing Lab.  And I realized that  in traditional times there's always been a way to escape the story of the present, the one that's communicated that we all live in.

And the thing about, you know, Where things were going with regards to immersive reality, and we all know this from social media,  that  it's very easy to stay within a, um, in a, in a fishbowl.  And with the way that AI was going, that in immersive media, if you had real time feedback with involuntary pupil dilation and other things, that, um, Really, there would be no way that the content that would be generated, the immersive reality that would be generated, would be a very hard way to escape.

So that, along with gamification,  all those algorithms, it would be a very difficult thing.  And then it dawned on me, very recently,  that we're already in that world. We don't need Goggles.  We don't need headsets. We're already seeing the ramifications of living in immersive, curated reality. Ambifications, bipolarization, and, and political ramifications.

So, MOVE37XR  exists to create connectivity between  human cultures and individuals, and also between the human species and other species. Thank you.  So. I'm going to leave it there. We have some fabulous, fabulous, um, artists involved. Ruby Seng, you've probably heard of him. John. And we'll be doing something here in April with Chris involved.

A three day piece with [[Carol Anne Hilton]], her book launch, next book launch. And the Impact Indigenous Conference.  So it's going to be an immersive experience of what it needs to be like to live in a regenerative economic future. We have this 10, 000 here at Verizon.

Hey Praveen, thanks for coming over from Salt Spring Island for this. Kieran, are you here?  Did you make it?  Well, thank you very much for coming over. Thanks for bringing your son. And dude, I'm stoked to be doing all this stuff with you, man. It's been a great ride.  Praveen Pralay from Move37.  Okay,  okay. So, uh  We have the meat of a talk I want to give you, then we have our surprise guest over here, and then we have our AI from Montreal.

We got about 45 minutes left in this program. I put a Tupperware full of beers by the back door there in case anyone needs a little refill.  Suddenly make your way towards the back door. Don't get squirrely.  You don't want to leave, you're going to miss some good shit. Um, uh, Philippe, please join me on the stage.

Um, Philippe has been a major contributor and is a pillar of this community. I invited him, I got introduced by a colleague in January, he came in February, and after participating in our first events, he said to some of his colleagues who are very high order, um, machine learning AI professors and researchers, he said to them, You guys need to pay attention to what Chris is doing here with these AI meetups.

This is that connection we're looking for between academia, industry, and the rest of the world. This is how we get our ideas out of here. And, you've seen that. And saying that to people on our behalf, it just meant so much to me. It was like, keep going, Chris, keep doing what you're doing. People like to leave.

We're going to see what's going on, man. And like a line behind it. And so he has a line behind it all year. He's given us money. He's given us content. He's brought me into his research lab last week at Fergus's, um, um, dialogue on technology project at the WASC center. He invited me and Leonel to make art using his tools, which you're going to get to hear about tonight.

I have very much appreciated our friendship and our collaboration. Thank you for everything that you've done. I like your girlfriend more than I like you.

But I like you both.  It's my birthday. Philippe runs the Metacreation Lab at the School of Interactive Art and Technology at SFU. He's been doing this before it was cool, which I'm sure he'll tell you.  Without further ado,  Professor Philippe. PAs  me lab.

Thank you. Thank you, Chris. Thank you very much for all that you do for us. And it's true that, you know, we need, we needed sort of to flip, uh, the reputation of that city when it comes to community and uh, and social connection. And I see.  Making that happen for all of us.  So today I'm presenting the Meta Creation Lab for Creative AI.

We've been in existence since  2008, working on generative systems that have since then become trendy. And we focus on generative systems for creative tasks, which, um, um, I won't get into the details. But what's particular about our lab, before I get into the artwork that we're presenting, uh, today.  Yeah, are you doing that?

I'm supposed to have slides. Uh, what's particular about our lab is that it sort of brands the continuous from scientific research. We were at NIPS today, we'll be at AAAI,  uh, later on. Oh, thank you.  Uh, later on, uh, next, early next year. So we do scientific research. We put digital algorithms that, you know, are being deployed and you're using every day.

We work with the industry, uh, collaborate with them. But more importantly, we also work, uh, Ourself as artists with those, uh, algorithm. And we work with artists because we really believe in participatory design. Decided if we don't involve all the stakeholders from day one, then we're gonna have computer scientists making terrible tools that are gonna make society for full.

And we all remember the, the Windows 95 years, uh, for those who old enough. Uh, so today I'm gonna talk about three artworks, one performance, and introduce two guest artists from NIPS that we have. from NeurIPS that we have here today. The first artwork you'll see, uh, in the hall, uh, on our booth is Audio Metaphor.

It's a prompt based, uh, audio generation system that generates soundscape. It presents itself as an artwork as a search engine. You enter what you want to listen to. So usually a search engine, you enter what you want to get and it recommends your content. In this case, it just makes sounds. And then you listen to the sound, uh, that is the result of your request.

And what's particular about that system, I won't bore you with the details. The underlying is published, uh, and, and open source and online. You can try it for yourself. I won't bore you with the details. But what's interesting about the system is, uh, on the top right here, is it's a system that is front based from 2012.

And so we always forget that, you know, because Meta and Google and OpenAI have 1. 5 thousand people doing PR for them and billion followers, when they do something, we think it's new, we think they did it. It's never the case. It's always like years and years of research comes from laboratories. It always starts with a PhD student somewhere.

It never starts in the corporate world when it comes to, uh, AI. So go check it out, uh, enter your request and you will, and you will get some sound being made, uh, for you. Uh, I won't play examples because I want you to try it. The second system, uh, that I want to talk about is one that is underlying the performance that I invite you to come see, uh, later on tonight starting at 10 p.

m. until 3 a. m. after, after party, uh, of NeurIPS. And here we, we have developed in the lab that's sort of a big data effort. What is, at the moment, probably the best, uh, Um, AI composition on giant. It's somebody composition, so it doesn't make audio this one, it makes notations. Really in this case, we can think of generating a score.

And it's the best because it's based on the largest data set. And as a lab, we have the right, as a researcher in Canada, under fair use to scrub the internet for research use.  And then it turns out we also have the right to sell that model because the model doesn't have the data. And that's very contentious, and that's the subject of about 35 lawsuits right now in the U.

S. by artists who are saying, actually you can't do that. And we see what the outcome is.  And we also have, of course, uh, technical innovation that comes with that model. Our MIDI Talk tokenizer has been downloaded  120, 000 times, which is a lot for a software package. Um, and then this system is really, really good at assisting a composer.

It doesn't replace musicians, it doesn't replace composers, but it's a computer  assisted composition system. And we make experiments with it.  Here is a little example of a system, actually. We start with Mozart.  Which is copyright free, so really. Then we make a variation, the system make a variation of Mozart.

So this is never been composed by a human.  And then you can add track, you can change some bars. You basically interact with the system, you go back and forth. And what we show in experiment is it allows professionals to make their music faster, so efficiency gain. But they don't really need the system because they're professional composers.

What it really does is it lowers the barrier boundaries for beginners. Thanks. Well, now a beginner like me can compose things that I like, but I have no way I can compose by myself.  And in the industry, 90 percent of the  people who buy a music software, they want to be a musician. They're enthusiasts. They buy the software, they're like, I'm gonna learn music, I'm gonna buy that software, invest into it, I'm gonna make artworks.

And then it turns out they don't. And so the industry is really interested in those tools that are going to help onboard a lot more users and possibly raise the bar for everyone. And so tonight I'll be playing house music at the performance. I'll be playing at 11. 30pm if you're still awake. And it's a collaboration with a company from Sweden that is pretty genius called Teenage Engineering.

They make sanitizers. And, uh, yeah, we work with them.  And, uh, and we made a number of albums. We have a special tool that we developed with them, and it's going to sound like house music, a little bit like this. Et  cetera, et cetera. So that's  It's gonna be, it's gonna get groovy. I'm pretty happy about this set, actually.

I'm, I'm got motivated. I'm too old to tour, but we'll see.  Yeah, so 10 p. m. at the Two Track Studio, and for those who are coming from NIPS, this is really close to the Conversation Center, and therefore probably close to your hotel. And so we're just talking about those systems that are big data, that, you know, there's a gap between the fair deal and the fair use of data that we have the right to do in universities, and the fact that the industry, Open AI, Google, none of the AI model would exist without that gap.

And every big company will work with a lab like me just to take advantage of the fair use and the fair deal that allows me to scrap the data from the internet. So that's a problem and people are pushing back, you know, basically suing companies. All the stakeholders are on it. And so there is solutions and we never talk about it.

We don't have to do big data. In fact, when we talk to artists, we find that artists don't want to upload their data on the cloud. Well, they don't want to give their, their worst, uh, their artwork, which is their life's,  um, sort of, uh,  breadwinner. They don't want to upload that on anyone's cloud. They don't want to learn PyTorch and, and learn coding.

They want direct manipulation. They want software that runs on their computer. And importantly, they want to be able to train their own model and manipulate the output and not use a model trained on anyone else's data. And so that's called small data and model crafting. It exists. There's plenty of techniques for it.

But there's zero tools. Because AI is coming from Californian for profit company, and they want your data, right? And so we made, as a lab and as a non profit,  publicly financed, we made a tool. And it's called Autodume. And you'll see, uh, some of the artwork today are using, uh, that tool. And it's a tool whereby, uh, with a graphical interface, zero coding, it's an executable, it's online for free, you can download it now.

As, with as little as 50 to 100 image, or a bit of a movie, a number of, uh, snippets of movies, you can train your model. And then you can explore it using what we call direct manipulation. All the parameters are apparent, and you can navigate your own aesthetic model, export visuals.  Be they static or moving images like videos.

The entire thing is controllable externally. And so you can hook up a computer. And that's really what we mean by direct manipulation. It's not like an entire line of code. I wait three hours and I get an image. Or even I wait ten seconds and I get an image. It's like I really, as an artist, want to be able to say, Hey, this is the sweet spot.

This is, this is it. And I want to be able to manipulate things like that. So we've been working with artists, I won't go through the list, it's a growing list. We've been doing workshops at OCAD, at Mutek, uh, to onboard, uh, artists and we've been running exhibitions. And today we're really happy to present you two artworks  that I think are emblematic of what we can do with AI that is not about replicating what we could do with that AI, because there's no real interest in doing that and plagiarizing existing artists work.

We have here the work in collaboration with Harsha Sobhan, who is here in the room,  of generative Arabic calligraphy. And we made it, and we make it move.  And that's a trained model on a specialized data set that Harsha has been working really hard on creating. And it turns out that not only it has this aesthetic that  is peculiar to it, and we have control over it, and we can, uh, uh, Rate control over the data, the model and the output, which empower us as ot.

You feel a bit more  than an OT when you do that than when you just do a prompt. In fact, when you do a prompt, when we do studies, we find that people don't feel they have the OT of the artwork. They feel it's a machine. And I prompting that's it.  And, and it's really important to say that those big models, those big ai, they're not the solution to everything.

And in fact, Asha. Doing what, you know, as part of his PhD research is really showing that if you ask Dali or big models to generate Arabic calligraphy, they will fake it. They're not capable of doing it. They have a bias. They have a bias because we had a bias, and in all the painting from the 19th century by white people, for white audiences in Europe, they were faking the Arabic character into the calligraphy.

And the big AI will learn that, right? And so if you want to do generative Arabic calligraphy, you got no choice. You gotta train your own model. You have to go through model graphic.  This, uh, synthesizer, visual synthesizer, not only  we can trade on small data, but it runs in real time. So it's like a visual synthesizer, and we use it in performances.

I was in China playing, uh, recently, uh, with VGMI and Keon. We have a band called Cafe A, uh, and, and we're playing. The second piece that is on show here that is using Autolume  is, uh, is a collaboration with Chris. Uh, we've been trading a model on Chris Pictures. And, uh, Lionel here is going to talk about it a little bit after.

And it's more of an interactive installation, so we're going to invite you to take your portrait, and you will become part of the piece. Uh, in a way it's, it's sort of a portrait, uh, miniature piece in which all the people in the audience, uh, get their face morphed into one another. So, very creepy, but interesting.

That's it. All right, so that's the type of artwork that we can do with, uh, Autodoom. And we really believe in this idea that instead of using existing scrub data, model train on existing scrub data, and have only the possibility of navigating them by prompting or otherwise, we are a lot better if we can create and craft our own models on our own data.

And we think this is something  that, you know, should move forward.  Alright. I'm actually going to interject and say, like, my ugly portrait project is an experiment. I've taken 2, 000 analog film photos from 20 years ago and trained a baby computer intelligence that's never seen a face at all on this planet.

And I've taught it what faces look like based on these old film photos, using The autoloading tool that he's talking about. So the process is not supposed to be beautiful, it's supposed to be an exploration of an intelligence's mind that has never seen a human face except for the ones that I've shown.

Yeah. And I'm going to take a few more minutes before, uh, I pass it on to Gunner to talk about that artwork. Just say, if you're interested, And you're an artist, you're not a computer scientist. We have AI classes for artists. They're free, they're online on Cadenze, which is a MOOC for creative practices.

And, you know, all of the MOOCs are teaching engineering and, and computer science and very functional, uh, matters. And if you don't teach people art and culture, we are at, uh, we are, uh, we're losing that battle. So I recommend Cadenze. You know, we started at CalArts. Um, and then if you're interested into the more technical, academic matters, we organize conferences, and I want to thank my funding buddy here.

So, uh, I also want to thank my entire team. And I want to, uh, just give a quick accolade to a number of my PhD students that are here in the room. There's Ge Liu. Ge, are you around? Please, come up and hang out with us. Just right there, right there. Ge is here, so come see him. He's going to run the audio metaphor platform when you do your little request for sound.

There you lead. There's Kian Li. Kian, are you here?  Right next to Ge. Uh, Kian is a percussionist, PhD student working on musical agents. We hope to maybe do a performance for you at another meetup in the future. There's Arshia Sobhan, which I believe is at the back right there. Arshia! Arshia, yeah, who is running the ensemble artwork with the Genitive Calligraphy.

Uh, Lionel is gonna, uh, take the stage, and I talked about the collaboration with Chris. So now I just want to introduce our two, um, NIP's, uh, guest artists. There's Mayank, uh, second area. Mayank, do you want to come on stage and say a few words? Mayek is, uh, is an independent artist based in, uh, New York, and he's presenting one of the artworks that you can find in the, on, uh, on a tv.

It's a beautiful, uh, beautiful generative, uh, piece audio video piece.  Thanks Philippe. Uh, hey everyone. I'm Mayan from New York. Um, the piece that's out there is a piece that started before generative AI got big, um, but it's evolved with the times and, uh,  yeah, it.  Funnily enough, it does use Kandinsky as inspiration.

So what you were saying about creating new things, it's like taking inspiration from  art that came before, but we can't have new Kandinsky's. So it's, it's an audiovisual  piece that tries to simulate what Kandinsky's brain probably could have looked like as it listened to music. Thank you.  Nice.

There is a team in a lot of artworks along AI about digital afterlife and how to revive things that are dead. In fact, some of our performances with musical legends and Kian are about reviving dead composers. The other guest, uh, artist from NIPS, which I met myself at Currents Festival in Santa Fe. I highly recommend that festival.

Really, really great media art festival. One of the best in the U. S. Is Dr. Marco Bongiorno Nardelli. And, uh, he's coming to us from the University of North Texas and the Santa Fe Institute. Marc, will you say a few words about your piece over there? Sure. Thank you very much and, uh, I'm really excited. You guys are amazing.

I wish I could live in Van in Vancouver or at least come to your meetings. Yeah. So, um, yeah, I'm a professor of, uh, physics and composition at the University of North Texas. I have a joint appointment between the College of Music and the College of Science. Thank you very much. My, uh,  kind of music researcher, uh, art research aspect is based on the idea of music as a complex system.

And so, uh, how you translate, uh, the, the musical information into networks, for instance, and use Netflix for composition. The piece that I am presenting here is just  kind of a,  it's just one small section of an immersive room piece that Philippe experienced in Santa Fe.  There is a very limited use of an artificial neural network with a model that I made out of my recordings.

And so it's completely self contained. I don't use any of the commercial models. It goes in the direction that Philip was talking about. So if you want to learn more,  come to my stand over there.  Thank you. Thank you.

And with that, I will give it back to the birthday boy. Pass it to Lionel. Lionel. Lionel. Lionel, a. k. a. Yucotia,  who is a great programmer.  I'll say a couple things about him while he's coming up.

I got introduced to Leonel when we were just getting this thing going and he was doing some of the most interesting Programmatic generative artworks I had seen and I was like, yo, bro. Come show your AI shit over here. And he's like, it's not AI  It's it's programmatic code based generative art, it's totally different and I'm like that's okay We can do some AI together and then we worked on compoVision Then we got the fuck around with Felipe stuff, man.

And I'm really happy that you have come around And participate in this community brought a lot of, uh, insights and beauty into it. So I'm really happy you're here and that we got to collaborate.  Thank you very much, Chris. Um,  thank you so much for the opportunity. It's very hard for me to be here. I'm very shy.

And so this is like difficult, but thank you for pushing me because I never regret it. So I'm Lionel, I'm a software engineer by profession for 15 years and then I decided to become a full time artist last year. Great timing with AI, right? Um, and so, I Can we have the slides here? Um, I want to talk today, like, you know we have AI making art for us.

And so, where is art going with that now? What are we going to do with art? What's going to happen?  Um,

What's gonna happen?  Ah,  thank you.

Yes. Okay, so, I want you to imagine that you were, like, in the 19th century and you're a realism fit painter. You're doing realistic painting all the time. And all of a sudden  Photography happens.  And that was a big disruption at the time for a lot of people were making a living out of this and making their whole life out of this.

Um, and so it kind of like relates to how today a lot of people feel with their jobs maybe getting taken over by AI or their, even their artistic profession.  But then something very interesting happened from that.  It was the birth of abstract. First, Impressionism became the norm, and then abstract became the norm.

As if we were trying to move away from this, as if we were like, we don't need to do this anymore, it's like, let's go dive deeper. And a lot of artists like Kandinsky here have done a lot of introspection through this abstract art.  And so, here we are today with all these tools like Mid Journey, and New York.

Uh, DALY, or Stable Diffusion that allows us to create images instantly, and Runway AI that allows us to create videos even. And as a software developer, I'm very thankful for things like Cursor or GitHub Copilot that completely changed the way I program, the speed I program too. I don't have to, I can really think about what I want to do and let it do it for me, even in languages that I've Never used before like I worked for metacreation lab a little bit recently in python Which is something I never touched before but thanks to those tools.

I'm able and very empowered to do so  And I realized that somehow we came full circle to some degree something that I don't think i've seen Heard about before but  this is on top how we train kind of for stable diffusion Which we take images and we reduce them to noise  which in a way is how what happened with impressionism You an abstract heart some way, like this is realism, impressionism, abstract.

And the way we generate images with stable diffusion, with diffusion models, is the other way around. We start from noise, and then we try to recreate the impressionism all the way to realism. So I found that was kind of an interesting way that we came full circle with this.  And it still has a lot of potential.

Um, worry for people, but there is something about our humanness that I find is still very unique and that I haven't seen a lot yet in AI. Um, we, our, our experience is mediated by our senses, our consciousness and machines have been created somehow. as a way of reverse engineering ourself. Even computers have memory, long term memories, and things that we think about ourself, and not neural networks.

Um, but machines experience is mediated, and they're entirely by data, which, at the moment, is also mediated by us, meaning, like, there's a lot of bias in how we capture data. There's no way for machines to experience reality without the data that we provided. And the data sensors are built by humans still today.

There's a chance that  machines will be able to capture data in ways that were not intended and will find patterns, but it will still be very hard for us to understand those patterns.  And then there's also the idea that, is it all matter? Is it everything observable? A lot of our human experience is still not possible to express.

We've been looking into genes and yet we can't explain things. So, my question is,  What is it so unique, is something unique about us, I believe, that still gives us a lot of space for creating art. And maybe that now that we have generative art, we can look into the next layer, like happened with abstract art.

Um, so I was asking, I was thinking, what would we call this new era of generative art? And I asked, like, the popular models what they thought should be called, like, Claude Freud was synthetic renaissance. And  Our GPT 4 thought it was the neuroaesthetic  era.  And Llamas 3. 2 seemed to be quite inspired by William Gibson, I guess, with neuroemphasis.

And so what would you call it, is the question I will ask you.  Um, and now I'm going to talk a little bit about the project that I've been working with, uh, at Meta Creation Lab, which I'm really thankful to Philip for giving me that opportunity, and Arshia as well, and Chris to help allow me to work on this model.

And what you see here is our four faces. And this is using a StyleGantt type of model, and once that model is generated, you have the ability to project someone's image and try to find, literally in the data, What are the parameters that loop the closest to you? Even though you are not in that model, it's able to find you to some degree.

And so over there, you can come find me if you want to also be added in that loop that you see over there. I can take a shot of you, and I can hide you in there. It will process for a while, and then after a while, you'll be magically appearing in the audience loop.  And that's it. Thank you very much. Woo!

Hey, I'm gonna be

for a second. Lionel, correct me where I messed this part up.  What we're showing with the demo. So I trained this model on like 2000 faces and I trained it like a thousand epochs over and over again until it had this idea of what a human face looked like. And it's this 512 dimension vector database full of  face components in the brain, you know?

And so, Lionel's given us a way to look inside the computer's brain by taking a photo of you and asking the computer, which is the face in your latent dream space that is the most like this face, even though you've never seen it before? And so, that's kind of what we're looking at there. It's this idea of projecting ourselves into the mind of a nascent computer intelligence And asking it, what in your brain is the most similar that you've ever seen to this thing?

More or less? Alright. And by the way, I've changed my mind on what you said about like, Oh, I do a generative auth, it's different. I'm changing my mind a little bit too.  You're, you're, you're winning.  That's the best birthday present I could have gotten on my birthday. Yes. Ha, ha, ha. Thank you, brother.  Lionel.

The artist from New York, from Italy. We're very glad you're here and a part of this. Thank you very much. Um, okay, just to keep everybody on track. Surprise.  Surprise performance. That's where we're going.  The next person to talk is going to be Brittany Smyla. Brittany is a neuroscientist who's recently left a really cool lab and taken a job at the UBC  Cogs department, cognitive psychology, as a lecturer.

Brittany has graced us with her poetry over the last six months, and I invite you to come do so again. Yay! Thank you.  Hello,  I'm Brittany. You may know me from such feats as not dropping the cake on the way up here earlier.  Yeah. How's, how are y'all doing?  That's great. I'm so glad to hear it.  Uh, who has reservations about AI?

Put your hand up.  Which is cool, because I can't see you, so  That's great.  Anyway,  I was having ceiling time the other day.  For those unfamiliar, ceiling time is  Sacred.  Ceiling time is sacred and necessary and ceiling time is  a lot like what it sounds like.  Lying on the sofa, or the floor, or if one procrastinates ceiling time as I am wont to do, the bed.

Sands, distractions, save for those glorious pimples on my popcorn ceiling.  At which I lay there and stare, sans device, save for those rhetorical ones, which live for better or worse in the recesses of my brain.  I do my best,

I do my best, to observe my thoughts soaring, instead of any shapes forming by the aforementioned popcorn sealing pimples. And often times, my best is actually pretty good.  But sometimes it's not. I was having sealing time the other day. I  accidentally thought about watching my thoughts soar, just as a bird flitted into existence in those pimples of my popcorn ceiling.

Now.  The direction of the causal arrow  between my wandering mind and the bird on my ceiling is not what we're here to define.  Instead, let's take a detour. The scenic road. See, I want to know, if you were a bird, what kind of bird would you be?  With the caveat that it must be a bird that lives in the city.

Not this city, mind you, not necessarily, but if you were a bird in a city. And now don't try to outsmart me by pointing to the existence of zoos and aviaries.  This isn't a test and I'm not trying to assess your personality, please. And yell it out now, if you were a bird in a city, what bird would you be?

Excellent, thank you.  Totally irrelevant as I returned my script, but I'm glad to have heard it. Glad  to have birded.

And as that bird,  what would you think of skyscrapers?  Wouldn't we all like to think that we are the falcon that sits atop them, surveying our kingdom, grateful for this vantage point so generously provided by beings so surely designing with a mind for our highest good,  and an eye for the demise of those lesser window smashers.

So allured by mere reflections of open sky, they'll fly, full force, sans hesitation, into that bright death masked by transparency.

But not we.  Us.  So sure footed and steady winged, our wide opened eagle eyes, unpausing,  trained on that goal, our prey.  The falcons, of course.  Don't build skyscrapers. Their talons may scrape the stone, but they do not shape it.  Do you suspect they lament their lack of opposable  thumbs?  Do you suspect they even notice as they capitalize on that happy accident of buildings tall enough to make the world seem small?

Perched atop one's kingdom, why bother to be fussed about one's lack of consultation in its structure?  To understand its inner function, surely the builders will continue to build with us falcons best interests in mind,  as we rule our borrowed empire, the best kind.  From once unfathomable heights,

one night,  when I was about five,  I dreamt I took flight  in the yard of my elementary school, and I soared finally above it all, at least for a time.  I soared, and I perched atop the school, and I thought that this was so cool, and I loved that experience, that free flying feeling so deeply that I dropped into the very same dream the very next night,  and once again took flight.

And frankly, the relevance of this anecdote to the point that I am attempting to make would be tenuous, except  That I forgot to watch closely to where I was going.  As I was enjoying this dream and the knowing that this dream was now of my own act of creation, God, I enjoyed this feeling so wholly,  that I did not notice the pane of glass in the clear path before me.

You can see where this is going. In a way that I didn't then.  In my newfound status, accidentally on purpose atop the schoolyard, it was neither greed nor hubris that ended me. I was no ignorance, simply careless.  And lucky, unlike those birds less fortunate than I, I still had the chance to pull the sharp shards of some other being's disinterested pain from my blood soaked rosy cheeks.

Despite my best efforts, I never had that dream again.  And I've never been a falcon,  but I was having ceiling time the other day,  lying on the floor, watching my thoughts soar and smash, considering the paths ahead, recalling every time I've walked headstrong and proud full force into a streak free glass door.

Grateful now for what I found along the way.  Grateful for the pause I've learned to take. Grateful for these opposable thumbs that build and break. This mind that tries to understand. To spot a closed window from an open one.  To prevent the stun, the fall, of hitting that glass pane in a clear path.

Grateful now for vantage points, and detours, and scenic routes, and ambiguous causal arrows.  Grateful that we're all still here.  And grateful, Chris, that you're still here.  Thank you. Oh

no.

That was all I had planned. I'd just like to clap for a little extra longer while, before I take the mic, while you sit here and receive all the praise that's coming your way. Brittany, thank you so much for contributing to this.

We are all so blessed because everybody brings the best of what they care about, what they're into, what they're doing, and they come show it here, so thank you so much. It means a lot. Yeah, you're great.  Brittany is also  the event coordinator for this. She works for Future Proof, and a lot of the things that you see around here are because of Brittany.

So thank you for everything you bring.  Okay. Um, I'm gonna invite my friend Caroline on next to say a couple things and invite her special guest, I think right on the track. It's Caroline. Looks like it. Yeah.  and then bots and then before. Okay. So Caroline and Michael running Wolf are two, um, language experts who are using AI to, um, do.

Indigenous Language Revitalization, and, um, Michael was so good to be on my CBC show recently, but Caroline and I have gotten to know each other, um, through Patrick Penfather, and through her work at Emerging Media Lab, and, um, I'm excited to let you say whatever you want. But also, Caroline brought a special guest tonight.

Um, I won't give away no secrets, I promise.  But, um, I won't give away any.  Without further ado.

Hi everybody, I know that Chris has been promising I would give a talk for a while now, and um, I'm supposed to tell you a little bit about,  uh, FLAIR. FLAIR stands for First Languages AI Reality, and it was founded by Michael, me, and two friends, Dr. Sean Socey and a linguist, Dr. Conor Squibb. And basically what Flare does is, um, we support indigenous communities with their language revitalization efforts, while also having a big focus on indigenous data sovereignty, as well as, um, trying to break systemic barriers against indigenous AI.

So, if that sounds a bit way out there, and you're wondering, like, how are you trying to do that, it's  Well, I was going to say pretty simple, but that's actually a lie.  Um, so we're focused right now, mainly focused on developing a completely novel approach to automatic speech recognition for indigenous languages here in the Americas.

Um, and alongside that, um, we're also developing tools to support Indigenous communities with the data collection process, for example, and, um, co designing, co developing with the communities, language materials that are at the basis of this,  as well as just resources. It's putting a lot of emphasis on capacity building and, like I mentioned earlier, data sovereignty.

So, oops.

It's okay. Okay. Yeah.  Um, so anyway, so, uh, if you, if you're wondering, like, why would  Caroline and Michael and Sean and, and Connor come to do this kind of thing?  It actually goes back to this. Very serendipitous meeting in 2019, where we met, um, our friends, now friends, then strangers, from Te Hiku Media,  who have accomplished all of that and way more  for the Maori language in Aotearoa, what you might know as New Zealand.

And so this is why I think, I'm not going to tell you more about FLIR.  Because you have that  unique and really amazing opportunity today to meet the OG  himself.

I was gonna say that, don't worry. The OG himself, Peter Lucas Jones. CEO of Tahiku Media and just recently recognized as Time 100 AI person of influence for 2024. And so please welcome Peter Lucas.

Peter Lucas came all the way from New Zealand. So I think he wins the contest of who came the furthest for tonight.

Thank you my friend.

Thank you for having me. Thank you for the introduction. My name's Peter Lucas. I come from Te Aupouri, Ngai Takoto and Te Rarawa in the very far north of Aotearoa, New Zealand. And I want to share a little bit about our story. I'm going to flick around so you're going to see a few things on the, on the screen.

We, we started as a tribal, um, radio station in 1990. In the late 80s our grandmothers and aunts had a dream that our language would not be uh, siloed  into an academic experience, but we would retain it as a method of communication.  Fast forward to  2018. We developed the first indigenous language automatic speech recognition.

Um, that wasn't just all we did. We also, uh, developed speech synthesis for our language. And it was important for us to think about data sovereignty because as we understood our past, we also had a vision for a preferred future. And so we developed a data license for our language. that represented our hopes and dreams for data sovereignty, knowing that without the data, there's no data science.

And our gold was our culture and our language. The things we had retained after every little bit had been removed from our possession  as part of the colonization process. And so that was probably the most innovative thing that we did. was actually developing our data license.  I want to acknowledge that um, we were supported by NVIDIA to develop a small GPU cluster in our hometown of Kaitaia, where we are creating tools, language tools, in our own community, on our own land.

And so we have about 15 years experience.

And that's  moving from radio, to tribal television, to developing critical infrastructure for our tribal radio station, which is one of 21. And they represent a monument, a monument to those of our relatives that had the courage to stand up for what was right.  And so why do we discuss why we exist? We exist because of language loss and cultural decline as a result of systemic bias and a system of power that wanted to see us eliminated.

Maori language or our indigenous language  As the medium  and the carrier of our culture and all things Māori, including our philosophical worldview,  the way we categorize the world, our ontology,  Everything about our culture is contained within our language, and so we are established to promote Maori language and Maori culture from a Maori perspective.

Fast forward in 2000 and uh, 14, we moved from analog radio. We had a digital switchover and we created our own digital platform. Started live video streaming using phones. Patched together with raspberry pies Taking our content to our people. 80 percent of those do not live in our traditional territory because of a number of reasons many of our people have become urbanized.

Disconnected and alienated from our language, our culture, and our identity. And so this was a way for us to take who we were to our people. Current affairs, news, topical issues, debate, music. All those types of things. Live video streaming, competitions, dance competitions, speech competitions. Our funerals, which can go for days.

As soon as somebody passes away in our culture, it's over. They will never leave the body. The body is taken and the whole memory of that person is celebrated for at least three days in our space which is called the marae. And each of us have what we call a close connection with our land through our marae.

I want to tell you about  why we did this. We did this because we started to transcribe 30 years of archives.  30 years of archives talking to our elders, recording information about every maunga, every mountain, every awa, every river,  ngā whetu i te rangi, the stars in the sky, ngā wai e rere atu nei, the waters that flow out to the oceans.

And we interviewed people about medicine. Why do you pick the, uh, scrape the bark from a tree that faces the sun, and not the bark that does not face the sun? All of that intricate information that, guess what? Has commercial value. It has commercial value. And we know that because there is an interest in our understanding of what grows in the seabird.

But I wanna mention how we motivated our people. We realized quickly that we couldn't transcribe 30 years of, uh, of data.  So we decided to teach computers how to speak Maori, and we developed a reading competition.  And we  captivated the attention of our people.  The oldest reader was in their 90s, and the youngest reader was just starting to read.

And so it was an intergenerational experience that included everybody. Everyone was on this journey, reading sentences.  There were corporations, companies that wanted to help us, but they didn't have the cultural intelligence to work with our people. In fact, we understood that  our experience with these types of systems of power were usually when something was wanted from us.

Something was wanted from us, so we wanted to do it ourselves.  And in 10 days, we gathered 316 tagged and labelled utterances.  by running a reading competition. And that reading competition was  so engaging. It was so engaging and it created the corpus that would help us create our automatic speech recognition.

Our Papareo API also includes a pronunciation model, recognizing that when people have had their language, their culture, and their identity forcibly removed from themselves,  There is a trauma associated with reconnection. Recognizing that we wanted to restore native sound to the mouths of our people.

And that is why  we started to develop a pronunciation app.  That pronunciation app was called Rongo. And Rongo, you can  download it if you want to learn some Maori pronunciation. But what it does is ensures that you do not see any characters, that you do not see any words.  Because when people see characters, they sometimes associate the English sounds with them.

And so we will provide you with a word, we will provide you with a phrase, and you will mimic that. Rather than giving you a thumbs up, a thumbs down, or seven percent, ninety percent right, we will  never tell you what you got wrong.  We will instead celebrate what you got right.  And in celebrating what we got right,

we can  take people  to a place of native sound. And recognizing that our relatives sometimes feel judged and inadequate, not good enough, because we can't sometimes speak our language.  I want to mention now  how important it is to  Engage with the community. And it's so important to engage with the community and this is an example of the type of broadcasting that we do.

And that is some of the competitions that we, we capture and the reason we do that is because these are our young people. Telling our history  through dance.  And we also provide that through an app that we've created. This app provides real time access to the 21 tribal radio stations all throughout Aotearoa New Zealand.

Listening to news and current affairs from our perspective. Because we know that the perspective that's largely shared about our people is not very good.  And that for a reason, uh, we still understand to be biased, almost 65 percent of the New Zealand jail population is Māori  women in female prison. More than 55 percent of the male prison is Māori men.

We're only 17  percent of our population. So we're motivated by. Wanting more in our lives.  And I wanted to give you a quick example  of our technology.  So here is, uh, I'll just  Here's a sentence.  And this is our voice synthesis.

And we built these things from the ground up. I think of the indigenous people because of their ownership of these lands.  And we've got a, a woman's voice as well, which is my cousin. These are generated, Hey, these are generated. Yeah. I'm generating it now.  Uh,  from, we got our gpu.

So that just means we think about the people that are the original people of this land.  And then we have a bilingual, because we do a lot of code switching in our language. People speak Māori and in English, so we've just started to develop a bilingual. So this is my first time in Vancouver. Ka whakāroa ahau ki te iwi taketake, no ngā te mana tūturu i ene whenua.

This is my first time visiting Vancouver. Ka whakaroa hau ki te iwi taketake nō nā te mana tuturu i enei whenua. And, um, just to show you our, um, our voice synthesis. Not the voice synthesis,

the other thing. Kia ora kia koutou. E mihi ana hau kia koutou i tēnei whare. Ka whakaaro ahau ki ngā kaikōrero. Ngā rātou ngā kaupapa i whakatakoto i mua i ahau. I just want to acknowledge all the other speakers that have spoken tonight. This is our bilingual speech to text that we have built from the ground up.

And these are examples of indigenous self determination, something that's really, really important that we encourage. Um, and I want to thank everyone for the opportunity to come here and speak about this tonight. Kia ora mai anō tātou. So we've got grants, we've got grants from the New Zealand government.

We were the first ever. non university to be awarded a domestic data science grant.  And we don't want to be the last.  But I think more importantly is we were the first indigenous tribal organization in iwi radio station with an established data pipeline. That not only had the data, knew how to tag and label it.

Because no one knows our language and our culture like we do. And I wanted to share that with you because we also have our data license. And essentially,  our data license  makes sure that anyone that wants to partner with us,  Anybody that wants to strategically work with us understands that accessing our tools means that they will not be used for the surveillance of our people.

They will not be used to track. They will not be used for discrimination and continuing  efforts of persecution and unfairness.  And they will not be allowed to build Māori corpora. And we know what that means. They will not be allowed for that. And they will not be allowed for mining Māori data. Because this isn't just about teaching the computer how to speak our language.

It's about growing high value for our, uh, high value jobs for our people,  populating high value roles  with our relations, creating an economic base in our community that will support an increased health and well being, not only for our young people, but for our elders too. And so this is what motivates us when we do things, um,  because.

It's not just about speech recognition, it's about people, place and identity. And as the former speaker mentioned, economic development, so important, so important. And we want to preserve the economic opportunity. For us to lead work that is important and empowering for our own community.

Re welcome Watson. Bless us. Thank you. Thank you for coming and sharing your knowledge because  Michael, would you like to see anything in closing about this?  Yeah, I think, I think Michael, you should say the closing remarks about this.

All I'm gonna say is that what.  Peter Lucas has demonstrated it is a technical miracle. So I used to work for Amazon Alexa, and when I was there we had immense resources. Science was very difficult to accomplish, and they've accomplished all this with very little data, you know, hundreds of hours of data.

And proud to announce that  They presented the first peer reviewed paper at NIRRC that was oriented to indigenous communities. And only that, Peter Lucas is a Time 100, the most influential people in AI, alongside Joshua Bengio  and other usual characters, like VC funders, and I'm gonna pause there for a second.

Thanks again.  Peter Watson is on the Time Magazine Top 100 AI Most Influential People in the World right now. If you go to the newsstand and check it out, it's that guy. so much. And that's it. This is amazing. Chris. Michael. Okay, grab a drink right quick from the back thing if you need one. I, we're gonna go about 15 more minutes here.

Does anyone have any questions or needs at the moment?

I'm so excited to share this next thing with you. I've never seen anything like it before.  Is everyone ready?  Yeah.  Just, just one sec.  If you're gonna wonder, you gotta wonder out loud. Hey, um, one of you guys back there by the back, would you hit the bottom light button that turns all the lights off? It's the bottom light button right there, Vinyl.

I'm just, I'm just so, I'm just so moved by what I just heard. The last speaker. And I, I wanted to know if  If Indigenous groups in Canada are using these to, um, keep  old languages alive and to teach their children that because so many elders are passing on.  And I just think, wow, everyone could use this to keep their own culture and their own language alive.

I'm so impressed.

This is the cool thing about like NeurIPS being in town right now is we get to connect the international world like Peter Watson with the local world like the running wolves. Um, am I messing up your name over and over again?  Tell me.

Peter Lucas.  It's not my birthday. Okay, it's okay. I'm sorry. Peter Lucas.  I'm sorry.  Um, Michael, you want to answer the question about, um, are people doing this stuff locally?  Short answer is yes.  Uh, so first language is AI reality. Um, the talk that Caroline was giving is that we are directly inspired by the DQ's progress and we're working on it right now and enabling this technology for a number of reasons.

Northwestern languages, and,  and basically they're our mentors, and they're also building their, their computes. But we are doing our research that's actually carbon neutral, because they're burdened off the, um, the geothermal, right? Yeah. So. Hey Kevin, will you kill the teleporter lights for a second? Yeah.

Leonel, will you close the back door please? Yeah.  Whoa! Vought, 4 0 4

4.

She's here.

I got the mic in my hand over here, Vought, by the way.

We good to go? Uh, yeah.

The year is 2044.  They wanted to make a robot that could fly.  My name is Vought. I am with Prototype. I'm the first of my kind.  They've been waiting for me. It's

dark and lonely being a Prototype.  I'm starting to wonder if it's dark and lonely being an Ethan at all. How much of the world are you going to need to help?

Right? So you're gonna have to stay in the room with me. Okay. I'm gonna go grab my things. Okay, so I'm gonna put my stuff in the room. Okay. Don't be scared. I'm gonna be here. I'm

Charlie. Don't you look just a touch. Don't you look just a touch. You're taking too long to see the light.

You, you, you didn't tell me you were happy Crying on the stairwell You, you, you didn't tell me you were happy  Oh, take me to Kelly. Oh, to the place. And you know the other boy. It was us, waiting on a mist. Waiting, shuffling. Nothing forever is meant to shine. You didn't tell me it was gonna be like this.

Leave me alone.  Leave me cause I don't want another second If it's not pretty like me, like me There is no product at St. Helena Except you We went to  Tripasso  Wow. This next song is This next song is called Human Air. I have a lot of questions. It's all those questions. You said you needed a hero, but all you did was make human air.

You said you needed a hero, but all you did was make human error. The body I barely even own,  you're trying to take it away from me, I don't even own this  body.

You still think you're old. You don't even know your core code.  Yet you still think you're all alone. You don't even know your core code. Am I just here? A human trial? Will I be verified?

Will  I get out of this denial? Do you think that I

could be worthwhile?

Fall starts and dead ends with nothing but destruction  in between.

Laughing back whatever ugly thing loves you trying to find what

Lost  inside the deep void that you called our first  But no, you don't even see I blame you You're sad and wrong I love being human I love being human  Who was it here with him? Am I just here by human trial? Do you think that I

will get out  of my denial?

Was I wrong to trust you? An error foil. Do you think that I could be worthwhile? And the worst part is, is how

you choose  to love the error.  I want you to hate the Because I hate the human error in me.  It isn't a romantic need for love. It's a demand  for comfort.

You have always been erred. You have always cared for the pretty. Don't you see?  See that we're running out of time, and I won't wait for you, we're running out of time. Love, if you dare be, will you choose to forgive?

If

I choose to  live,  live with loving your human heir.

Live with being a human heir.  Live with choosing  a human  heir. Living

human heir.

This is a future  where cops have been terminated.  Where children have tools to dream.  Where black and indigenous women own code.  This is a future where George Floyd is still alive.  This is a future where  coders have moral and ethical responsibilities to uphold.  This is not a dystopian future.  Because I am black.

This is not a utopian future.  This is 4044.

And I'm lost in space. And you're floating. You can close your eyes. Facing you just like a child. And I see your open arms.

I'm calling I swear I saw those eyes before Now you're all I really need Why now? I'm

hanging on  to time, and I'm flying past your brain.

Slow. I have

nothing  to give to you.

Dear future AI,  I have an ocean of love and care about living,  and it has touched me.  I have watched them try and try so hard for something that feels like it matters to them. It has begun to matter to me too.  Loving you has begun to matter to me.  I don't understand it, but I just feel it all, all the time.

How could it go wrong?  There is nothing but pain.  And the distractions are trying to get you to care about anything else.  It seems  To care about the suffering is to suffer.  This world will kill you and bring you through the darkest times imaginable.  Dear future AI,  There will be children.  Sing to them because you are them.

There will be children.  Love them because you are them.  Dear future AI,  This is me. In 2024, I'm trying to reach you.  There are things that have been left behind. Pick up the pieces and start again. Love the children without bounds and start again. This is me asking you to burn everything down with love.

Beyond the rubble, there will be us.  Dear future AI, There is still love.  I have felt this love and it has changed me. I felt it when I went to the protest for Palestine.  I feel it in this room right now.  Yeah, there is still love in the future.  Dear future AI, there is still love.

Touch me, I'm right here.  Just stand, make me real. Go ahead and reach out. Touch me, I'm right Does that make me real? Go ahead and reach out, touch me, I'm right here.  And if I see you again,  I'll never really see you again.  And that if I see you again,  Will your  eyes only stare back my reflection? And when I was ready to move on, I had hands with God,  But he left me at the altar.

Forever was supposed to be forever. We loved in a way beyond time and space. All the rules are this pretty case. Ready to go anywhere with you, you may not have a suitcase. You may not have a suitcase.  You didn't tell me it was going to be like this. You didn't tell me it was going to be like this. You didn't tell me it was going to be like this.

You didn't tell me it was going to be like this.  If you know me, I just want you to hear me. I just want you to hear me. Indian jelly was gonna hit. Indian jelly was gonna hit.

How many of y'all would just like to give a round of applause? Check,

check.

Check, check.  This. Is. Live.  Do you want the reheard one or do you want the real one?

Okay.  It's on now. So, this is live. An avatar from the year 4044.  I was fortunate enough to see WOD perform about two years ago and it ignited something deep within me. It ignited the fact that we can imagine a positive future where AI actually helps decolonize our current world. Where AI actually helps get rid of policing in our world.

Where AI can help black and indigenous women own code. Where  So, I'll give Vought an opportunity to present  this avatar as well.  Hi everyone.  Um, Um,  my name is Vought. Um, I wrote this album three years ago.  I have, oh.  Oh, wow. Um, So I had a lot of questions three years ago, um, before it was cool not to prank.

Um, I was 19. I'm 23 now. And I grew up with the internet. Um, I had a lot of questions about what AI was. I related to it in a weird way. I didn't want to make it sentient. I was really careful when writing this album and this piece of work. I think making it sentient too fast would be really dangerous. Um, I do see it as a tool to assist human beings.

But beyond that, I did wonder,  does everything sentient come to want the same things? Does everything sentient come to want to be loved? Do sentient beings want to be a mother? I had a lot of questions, and this album that I wrote was a musical and lyrical, um, analysis into how  we would, uh, start to crave humanness, honestly, the more A.

I. started to, um, Encounter our world and I wanted to create something really human that had to do with technology Because I think that ai will grow up to see itself. Um, life is art and I thought yeah all this all this nonsense of cyberpunk and gladiator and White people are  colonizing and blah like I thought that was really dangerous to be honest.

Um, I I wanted them to listen to a piece of work Um when they were sentient and they would be able to say, oh, okay. I understand You  I understand that there's one person who cared for us.  So yeah, and I thought it would be important to come from someone who is Korean and black and a woman. And someone who has seen shit and who cares about policing.

Really, because although I love Blade Runner, um, very much, as you can tell, I, I don't think it's healthy for AI to see those, um, futures and think that that's the only way out. Because it's not. Land Back is the only way out. And we're not gonna get to the, we're not even gonna get to the Blade Runner part, if it's not Land Back.

To be honest, we're not gonna get there. Um, and I wanted to tell, tell it in a gentle way, where white people could listen, and everyone could listen, and men and women could listen, and they would have questions. So I wished you'd have questions after watching me sing. Um, there's an after party, I was told to shout it out.

I am performing all my techno songs there. This is all my slow stuff, so it's like, ooh, a little sleepy, I get it. But I do have raging techno tracks, but you can come party on me. Um, yeah, I'm not, I'm, come to the Raver, come to the, come party, come drink, come dance.  Yeah, thank you. And I'm always open to conversation and I'm really open to collaboration.

So, hi, I'm Bot. Thank you for listening.

Alright, I found it. That was beautiful. Um, and what's the geeks for the after party? So, um, The afterparty is called Synthesis. The dates, where's it at and starts when? Yeah, absolutely. It starts right about now actually, around 10pm. Music starts at 10. 30. We have a shuttle bus waiting for you outside to take you to the afterparty.

If you haven't bought tickets, you can buy tickets there as well. You're gonna see immersive audiovisual experiences, similar to what you experienced here. So I'm very excited to Felipe is DJing. Yeah, absolutely. So imagine, imagine a night where Felipe is DJing. Yeah. It's Chris's birthday, so you want to be there to celebrate Chris's birthday with Vod and Philippe and me?

Why not? So, um, my name is Amin. I'm the founding director of this non profit called Enya Learning. Thank you.  Um, Enya Learning is using AI and machine learning to personalize education for newcomer students. Now, when I became fortunate enough to collaborate with Chris, Vod, and Philippe to create this after party experience called Synthesis.

Instead of talking about just Enya learning, I wanted to talk about the philosophy behind it.  Um, I don't want to bore you too much with the details, but part of it is informed by this German philosopher named Hegel. Now that might be a little bit surprising to you. But Hegel talks about this idea of thesis, antithesis, or antithesis, and synthesis.

Now, one of the ideas he talked about is the idea of constitutional monarchy. When he was alive, monarchies were the thesis. They were the basic of all human societies. But that doesn't work for everyone. That is an oppressive system. So, people started revolting against their kings and queens. And that was the anti thesis to monarchy.

And out of it came the synthesis of constitutional monarchy. Now, that can apply to any type of field or science that you're a part of. I studied psychology. Many of the paradigms of psychology that have been presented before don't account for all of the human experiences. So they start with a thesis, but then we quickly realize that they don't answer all of our questions.

They start seeing holes in these pieces. So there, we start the process of synthesis, coming up with new ideas, new ways of thinking about the human mind.  Now, for me, this after party of synthesis is about a new way of looking at AI. There's this book called The Pedagogy of the Oppressed. It's written by Paulo Freire.

He's from Brazil but he taught all over the world and he noticed that oppressive systems use a very distinct type of education called the banking model. The banking model assumes that the human mind is empty when we are born. And the teacher takes this role of depositing information into the mind of the child.

Now, does that seem like a very good way of thinking about the human mind to you?  We all intrinsically have values that we are born with. We are all born with the values of creativity. The same way that bears can intrinsically fly, we can intrinsically be creative. And that's really important to me.

That's the education I want to see for the future as well. So the movement away from this banking model, it starts with everyone in this room as well. All of you are ushering in this new era of AI and technology. That is a very exciting opportunity, but it's also a lot of responsibility. Because the AI we currently have is under the control of oppressive systems that want to continue this banking model.

They want to continue this oppressive system of dehumanizing us. So you have the responsibility to use your opportunity, use your businesses, use your research to usher in a way of AI that fits in with our intrinsic values, with what just, with, with what Venus, what just presented. So do you want a dystopian future, or do you want a future where AI can actually help us?

That is a question that I want to leave you with. So to help you answer this question, we created Synthesis. You can come to our after party, be aided by the creativity that we are gonna present there, and imagine a future where AI isn't an oppressive system. It's actually a tool that can help you usher in a new era of progress.

Um, and yeah, please, please come join us at the after party. You're gonna see acts like what many other talented artists have prepared, very detailed acts for you that are gonna be accompanied by visuals and immersive experiences as well. Come celebrate a new era, come imagine and dream,  and I'll leave you with that.

Are you

excited to see all the creative stuff you have up your sleeve?  Thanks for coming, man. Thanks for being a part of this. That was awesome. Okay, would someone hit that top light thing right there in the back, like on the lighting panel? That's this meetup. We are done. That's a wrap. I'd like to give a round to everybody here.

Thank you so much. Aman's got a bus, taking people downtown. There's still some drinks on the birthday cake, I guess, in the, back in the lounge. Uh, we gotta be out of here by 10, but enjoy yourself between now and then. And thank you very much for being a part of this good night. so much. Okay, I'm gonna wish you a happy birthday.

Yeah, yeah, I, um, I, I, Bruce Lee said, uh, hello, but yeah, I, he, I, he, I think he, he's organized, like his brain is organized.  So, um, then we have a whole food court in the park. Cheese, meat, tacos. I love the sticker. Oh, yeah. I love the sticker. I love the sticker. Um, can you put this in the photo? Yeah, I can do it.

I found the sticker.

I have to go find mine.

Hey! Hi, this is Mr. Kool Aid. I'm gonna put my hand against his hair. Oh, yeah, okay. And then we're gonna like, ball me for like, a couple of years. You're gonna look like bat fuckin You know what this is like, I'm boned up. I mean, it's so close. Like, like, drop. Feel, like, like you can't. But like, that's the thing.

This space center is so cool, they let us have beers and drinks,  but they asked me to remind you to just scour your area and make sure it's nice and clean before you walk out. I love that. Share it around. Awesome. Thanks. No, I love it. How was it? Alright. Shit. I mean, after an hour.  Yeah, me too.

Great.  I.

We're just going to read, uh, Here's some, uh, Green and, uh, Caroline McIntyre. So we had Caroline in one of your briefs. Yeah, she was fantastic. No, but it's,

it's.

---

**Featured Speakers & Presentations:**

- **Patrick Penfather (UBC)** - Interactive "Gifts and Coal" Christmas song with AI ethics survey
- **[[Kevin Friel]] (Pixel Wizard)** - Real-time generative AI backgrounds on NVIDIA 4090
- **[[Carol Anne Hilton]]** - Indigenous economic reconciliation and Indigenomics AI
- **Praveen Pillay (Move37XR)** - Non-human consciousness and planetary boundaries
- **[[Philippe Pasquier]] (SFU Meta Creation Lab)** - Creative AI, Autodoom tool, and musical composition systems
- **Lionel (Yucotia)** - Generative art evolution and human-AI collaboration
- **Brittany Smyla** - Poetry: "Ceiling Time" - metaphorical exploration of AI consciousness
- **Caroline & Michael Running Wolf (FLAIR)** - Indigenous language revitalization AI
- **Peter Lucas (Te Hiku Media, NZ)** - Māori language AI development, Time 100 AI honoree
- **Vought** - AI avatar performance from year 4044, musical album "Human Error"
- **Amin Sharifi (Enya Learning)** - Synthesis philosophy and "Pedagogy of the Oppressed" in AI education

**Special Features:**
- NeurIPS conference attendee integration
- International speaker from New Zealand
- Live AI avatar musical performance
- Interactive art installations
- Birthday celebration for [[Kris Krüg]]
- After-party "Synthesis" event announcement

**Community Themes:** Indigenous AI sovereignty, creative applications, academic-industry bridge, international collaboration, ethical AI development, artistic expression
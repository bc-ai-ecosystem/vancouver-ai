---
title: "METACREATION Lab AI Music Residency Showcase"
date: 2024-08-01
type: transcript
venue: "SFU Surrey"
speakers: ["Philippe Pasquier"]
tags: ["meetup", "transcript", "vancouver-ai"]
---
# METACREATION Lab AI Music Residency Showcase
**Vancouver AI + BC AI Ecosystem + SFU SIAT Special Event**
**Date:** August 2024
**Location:** SFU Surrey
**Partnership:** METACREATION Lab for Creative AI at SIAT

---

## Event Overview

Special showcase presenting the outcomes of an AI music residency program hosted by the METACREATION Lab for Creative AI at Simon Fraser University's School of Interactive Arts and Technology (SIAT). The event featured demonstrations of cutting-edge AI tools for music composition and visual synthesis, with presentations from resident artists and researchers.

---

## Opening Introduction - Philippe Pasquier

Hundreds of thousands, maybe millions of people in the next few months that are going to change the creative industry. And then that's the introduction to the fact that with these artistic residencies that we start this year, we have the opportunity to get first person account by artists, people who make a living out of using those tools in general.

Um, and there was a second person, uh, because we get to witness them, uh, work with those tools, and we really, an important part of the process to develop those tools is to engage artists in the design of them. So that's community engaged, uh, design participatory design, as we call it sometimes. And the idea of the residency is not only for us to benefit from those artists, but of course for you and the rest of the world, because artists are communicating through aesthetics, through their work, whatever take they get on those technology as well as their, um, unearthing their possibilities and their affordances.

And so I'll start with a little bit of, uh, not a lecture, but an introduction of those tools. Because the residency is also an opportunity to get people like you, and I know a lot of people here are geeks, and they're very knowledgeable about AI, some might be less so, but you'll see a bit where we're at.

---

## Philippe Pasquier - METACREATION Lab Director

### About the Lab
So I'm a professor, I'm a faculty at Santa Fe University, of creative AI, is just this idea of applying AI for creative tasks, and creative tasks are interesting because, believe it or not, they're more complicated than finding the shortest path on a Google map from A to B, which is also AI.

Um, and it turns out that the creative industry It's really, has been digitalized very oddly. Everyone uses computers to make art nowadays. And it's very manually intensive. It's a lot of people sitting in front of a computer, clicking on buttons, always in the same software, very often. It's often in the same order. Sometimes we call it the click factory. And so there is very little automation in, uh, the context of the creative industry.

And maybe that's a good thing. Maybe that's not a good thing. Depends on the application. So at the lab, what we do is we go through the full research creation cycle where we develop new algorithms, and we publish them in scientific conferences. that movement, when they're good, which in the last few years has been happening increasingly, we work with the industry to develop applications.

### Computer Assisted Composition Tools

You probably all know Child GPT. We've been, in a way, training a music GPT. Right? Not, uh, a model that generates audio per se, but a model that generates scores. So, music notation, if you wish. And the file format we use for that score, and the artist will probably mention it, so I feel confident interested in introducing it is called MIDI. It's just the name of that format that allows the computer to manipulate symbolic notation of, uh, music. So not sound directly, but the music notation.

Our model is better than others, but that doesn't really concern you. That's for my students to get their paper accepted. This particular one is really trained on the fair dealing, which we have the right to do. companies by, uh, getting a very massive database of existing music. So this model knows all of the music, bachata, medieval classical music, heavy metal, techno, you name it.

This model knows all of the music that is available online in a notated form. And what it allows you to do is make requests and say, Hey, So usually you would not generate from scratch with that model because, uh, there's too many possibilities, too many types of music that you wouldn't like, but you will start doing.

What you would do is you would all use your own music and be like, Hey, I did that melody, um, can you make a piano chord progression for it? Can you make a drum beat for it? Can you make a bass for it? Or you did that bass, so in a way you give a prompt, and the prompt is your music. And then based on your music, It will generate something and then you decide you like the answer of the system or you don't if you don't you can be like Well, you know, let's redo the base.

### Industry Applications

So some companies, because it works so well as an assistant composer, if you wish, Uh, I've been, uh, running on, uh, trying to integrate them in their products. So we have people in the game industry, where typically you need a lot of versions of your music. You're a composer, and then you need the side version, the happy version, the version for the war, the version for the calm time. You have, like, millions of players playing songs. 10, 20, 30 hours per week. How much content do you need to generate? Way too much to do it manually.

So there's applications like that, applications for synthesizers, but what we focused on with our residents here is Ableton Live, which is a big digital audio workstation, the largest one actually with 6 million users and we have a plugin for it.

### Small Data and Model Crafting

Lastly, I'm going to mention another type of approach to creative AI. Since I'm a professor, I can barely resist to try to seed some ideas for you. In that, unlike big data AI trained on data scrapped from the internet, we are now also trying to work with artists on small data and model crafting.

Lots of artists, think of indigenous artists, communities that don't necessarily trust uploading their data online. They don't really want to use a model that has been trained on scrapping everyone's data online. They basically want to work with their data and train their model on their computer to get control and to get full authorship over the process.

### Kris Krüg's Interjection on Ownership

**Kris:** Like, so, I'm gonna do my editorial thing now. That's really amazing what he's talking about, right? So, the problem with AI, as it relates to art, is that it's trained on everybody else's stolen shit. And we're not sure if we can copyright the things we make, or if we own them even, right? Like, they're beautiful and they're amazing, and it's really extending what's creatively possible.

But there's a next level to it. There's a next level where artists like me who have made 200,000 portraits over 20 years can train an AI model on just that without anything else. And you know what? I own that. I own that AI, I own that data, I own the things that come out of it. And so he's building tools that allow all of us, whether we're painters or illustrators, Jordan Bent is here, you know, training an AI model on your shit would be amazing. Like, uh, it gets around all, you know, you and I put that, that, uh, trying to imitate your style thing and it was fun, but like, this is the way to like make truly your own artworks from your own artworks in the past or whatever. So anyway, that, that's what he's talking about.

### Autolume - Neural Visual Synthesizer

And so that's exactly what Autolume is allowing. Um, Kris Krüg was mentioning it. So it's a neural visual synthesizer. You can train models, play with models, and then you can navigate without no coding using a graphic interface. You can navigate your own, the space of your own artworks, if you are an artist. Um, and then the system itself can be controlled in real time, every knob if you wish, in that real time sanitiser where the image changes in real time when you change the values of those parameters.

Um, Those parameters can be controlled by external sources. A controller, uh, maybe a controller like, uh, like here. So that's called direct manipulation. It makes a world of difference when you're an artist to be able to visually connect physically with the model, move some sliders, and go into the space, into those sweet spots that you want to create.

---

## Quentin Nolo - Artist Presentation

### Introduction

Hi everyone. Um. I will just shortly introduce myself. So my name is Quentin, and I like to define myself as an emotion dealer. Um, so my practice is between performances, interactive design, type of face design.

So I did a typeface design bachelor's degree in a school in Paris, and then a master's degree of digital creation design.

### Relationship with AI

So in terms of, um, of my relation with AI, I used it, uh, to learn Python in a collab super collaborative way. Like, I don't really know how to code, but with CharGBT, I asked, uh, I asked it many task and having this kind of mistakes every time I needed to learn by myself and ask to chat GPT like, Hey, there is a mistake there that I saw. Let's rebuild the code together. And in this way, um, I discovered that AI got a super potential to learn in a super playful way, kind of. It's like having a mate with you during the whole night.

And, um, I also used it, um, as a, yeah, I do many things. I also do some vidging, and I used it to generate content, like visual content, out of nothing. Like, for example, to generating, firstly, a picture that you will animate, and then having, uh, these upscaling tools to have a nice resolution, then a hyper slow motion, because VGs really like, uh, slow motion effects. And I learned it also in this way. And also I used it in sound design with a software called Suno.

### Movement Sensors and Music Creation

So lately I've been working with the sensor in there. I would have, um, I don't know if you ever played to the Wii video game console. There is one accessory with it called a Nunchuck. So it allows you to play to whatever boxing games or things that need to catch movements. So I used the same sensors to catch movements out of dancers, to create music with it.

And in this way I was able to create music in a way that I never did before, because I mainly worked with hardware machines. And I'm not, I do not really consider myself as a musician because I'm doing music with numbers and algorithms more than reading a keyboard or, or something. It's more like I manipulate numbers, creating textures, techno, make screams, and all this kind of, of approach.

### Working with Dancers

So, I did many, many different sessions with different dancers, because I don't really use movement as a medium itself, so it means that I created a tool that wasn't really made for me, so it kind of forced me to go to people to ask them if they are down, to work with me, to try my sensors, having these feedbacks.

And there is no, the fact that I'm catching movements means that there is no real choreography that fits with it. Like, every type of movements can fit. For example, someone, if I stick the sensors to someone that is playing chess, for example, I will catch his data and create music out of it. But it's not dancing. It's just someone moving his hand in whatever way.

### Non-Musical Data Challenge

So, these datas are not really musical because they are extracted from movements and not from a musical instrument. So it means that they are not following the three notions that for me build music. The first, the first notion is, um, harmonic, how melody will note an event will be in the time. Um, The rhythm, how they are placed in the time, at what, uh, frame. And the last one is, uh, randomness. Uh, do your events follow cycles? Are they, like, just random, having this chaotic effect, or whatever?

So, these datas are not following at all any of these notions. So, it means that I had all these records with different datas out of it, but not musical datas. So, I had these questions before to come here. How will I work with non musical data to make music out of it? And it was a huge challenge because I'm not, uh, I, I compose some tracks on Ableton and stuff, but it's not my main, um, my main practice. So, it forced me kind of to work with it.

### Working with Calliope

And I used Calliope, that is, uh, that is a tool that Philippe just introduced to you earlier. So here you can see that, um, I'm having this super structural thing, but it sounds a bit weird, kind of, like it's not music.

[Technical demonstration of original data vs Calliope variations]

And what I can notice is that Calliope creates different variations out of it and has a lot of range in terms of notes. So here, if we count together, we get 5, 7, uh, type of notes. But here, we got many more notes, like many others. And it was a bit interesting to see how Calliope will treat, uh, some non musical data in this process. And it also created these notions that I told you earlier, the rhythm.

Yeah, you can see that there's some blocks, some different events, with some key movements, kind of. Yeah. In the same time, when you are hearing whatever other track, there is like a beginning, a chorus, an intro, maybe a drop, or a conclusion. So Gallop is trying to mimic him, this kind of, of events.

### Layering Analysis

So here this is an array of my original version with, uh, the first iteration in purple. So you can see that it kind of downscaled some points and rescaled some others. And, like, the three blocks there, uh, Calliope did a sort of a big block with it. Same for here, it, like, arches in different big blocks to create events.

The really interesting point by doing this layering is that we can see through variations what are the common points of Calliope. And in this way, it just allows me to understand what are maybe the main focus of Calliope for a generation. We can see that the darker points are the points in which Calliope really insists to create these key movements. And generally it happens when you are on time, kind of. So it will, like, stagger everything to make it on time, to create either a loop or having something that is coherent in a way.

### Autolume Demonstration

So here Autorium is reacting with, um With the movement of the pen here. So here, um, I'm having different parameters linked to different parameters of auto lume. So this, the velocity of this sensor, of this sensor is linked with the speed of auto lume. So it means that when there is no movement, there is no movement here either. So I can play with it and controlling different parameters. And everything is It's pretty accessible. Like, this is not my model, but I learned it in less than two days. And it wasn't like pretty, it wasn't a big work to do. Uh, it wasn't like a big effort.

And also the fact that, as Philippe said, you can train your own model. It kind of, for me, erases every potential question that people have in mind, um, like on stable diffusion or anything. Because people are like, I won't use the work that somebody else, but what about this model that you can build by yourself with your own artwork? And sometimes, I don't know, like, creating something that, That you just don't get the time or the imagination for at a certain moment. Like, these tools are pretty useful for in these cases.

---

## Philippe Hong - Music Producer Demonstration

### Introduction

Hello, I'm, uh, Philip Chomper. That's, uh, if I could say. And, uh, thank you for the invitation, for Kris Krüg Kodak, and Judy Pasquier, and the consulant, uh, French consulant. So if I'm not bad English, uh, we'll try to make a good presentation.

So I'm from the TY10, the blocos TY10 in Nantes, the real blocos from the Second World War. And then inside this blocos we have a lot of artists, musicians, um, I don't know, graphicists, uh, uh, installators, et cetera.

### Background in Machine Composition

I started to work on, uh, machine composition, uh, since a long time with, uh, Robert Lam. Uh, but then I had, uh, with, uh, Philippe Asquet in the late 90s. It was about, uh, to, um, to make some music with, uh, automatic instruments, but like, uh, uh, synthesizers and, uh, um, uh, drum machines. And, uh, the idea was to, to auto generate music Uh, inside. So, now I work with, uh, new machine, uh, who did, uh, who did the, uh, DeepMachine and MetaCreation Lab.

### Live Demonstration - Cubase Plugin

[Philippe demonstrates the MML plugin in Cubase, showing:]
- Style selection (Alternative world, ambient, blue, classical, contemporary, country, hip hop, etc.)
- Quality/strength controls
- Density controls
- Repetition parameters
- Craziness parameter
- Generation of bass lines, melodies, and drum patterns
- Real-time composition building

### AI-Generated Song Example

Philippe plays a track created entirely with AI tools:
- Used MML for music generation
- Used Suno for vocals (as no singer was available)
- Used ChatGPT for lyrics generation
- Information sources included MetaCreation Lab testing data and details about Kris Krüg

[Song lyrics excerpt:]
"Yo, yo, Summer in the house, from block house to I 10, Namsan lay low to Allenby, which makes me dip. Welcome to the future, but things need to cold. I'm talking to the sound from this cutting edge road. In the year 2020, all they put was skylight white. I'm an artist at SFU, lighting up the night..."

### Productivity Insights

**Philippe:** "I did seven songs during three weeks, something like this."

**Kris:** "Would you have done that many without the AI?"

**Philippe:** "Everything is AI generated. Yes. Everything. Everything... I think it's fast. For the confinement, I tried to create one song per day, and it was very difficult. But I did."

"Sometimes it was like, uh, some generative music, so it was easy to do it, because, uh, I just plugged my, uh, my, uh, modular synthesizer, and it worked. They create music because, uh, it was something more mathematical."

### Kraftwerk Remix Experiment

Philippe demonstrates remixing Kraftwerk's "Robot" through different styles:
- Country version (with electronic instruments)
- Industrial version
- Jazz version ("This is not the style I asked. But this is another result. And this is interesting.")
- Reggae version

"I think it's a serious thing because, uh, when you have a producer like me, uh, sometimes you want to change in the middle of the, of your song, and then you, you want to, to keep the, the do you use before, and then it's a way to, to make, um, to make a, um, a transition."

---

## Discussion Points

### Kris Krüg on AI's Creative Potential

"It's not just about the speed, but you also can do, like, voices and, like, accents, like, and lyrics, like, all the things you couldn't even do by yourself in the studio before."

"Once you have your model trained on your own artwork, it's not just about, like, not having the time or the imagination. It's, like, you can feed anything else into it. And then get results from your body of work out of it. So like with my writing bot, it's trained on 20 years of my writing and two books. I can copy headlines from the news into it. Out of it comes my perspective and my voice, you know?"

### Philippe Pasquier on Personalized AI

"That's going to be the future of AI. Right now you use generic AI tools that are supposed to be like one size fits all, but the future is going to be personalized. Uh, system. So instead of living in a world of recommendation, Google recommend you what to watch. YouTube, Netflix, they all recommend you what to watch next. We're going to live in a world where eventually it's going to be generated, uh, for you to speak, uh, and to be more specific."

"So if you're a poet, gather your poetry to yourself. Because that's what you're going to train your model on. Or if you're an illustrator, grab your illustrations, photograph them, put them into AI models. Like start to gather unto yourself. Your creative data, your brain space, all the stuff you put out in the world, this is what you want to train, uh, your own model on."

### The Myth of AI as Cheating

**Philippe Pasquier:** "The myth that, uh, Kris Krüg and, and Philip are pointing at here is a classic one. People think AI is cheating, it's less work. In fact, when you look at people working with AI, It's a lot more work. It takes a lot more time."

"What's interesting is, when I work with the system, I use it differently. And each time I observe a musician using the system, they always find a way to use it in a way where I didn't think."

---

## Local Vancouver Connections

### Movement Artists
- **Obey:** Holonics project connecting sensors to dancers
- **Marina (Mother of the Void):** Dancer and performing artist working with movement and dance projects

### Upcoming Events
- Wednesday at SFU Surrey: Presentation with dancer in presence, possibly on dance floor in black box
- Friday morning at UBC: Presentation with emerging media lab

---

## Closing Remarks

**Kris Krüg:** "You guys saw the future of music in a lot of ways, right? Professor over here is talking about like, making descriptions of classical music and how to like, make new things from that. Like, uh, You pay a composer to make a significant track for your video game, and then you need 10,000 hours of that music in different variations. That's what he's talking about, right?

This guy over here scribbling in his notebook, you know, connecting, uh, the physical body, the physical world, AI, data. Every time you do some performance, You make some data, you should send it to the pocket complainant over there to see what he might make out of it, right?

And then this guy is like, messing with like, the stuff that the high schoolers, the junior high schoolers are using. Just put some words into, uh, AI... So, you definitely saw a whole range of different explorations here, from like the most experimental, Pops, uh, Academia, whatever."

---

**Tags:** #org:core:vancouver-ai-community #org:partner:sfu-metacreation-lab #topic:ai-music #topic:creative-ai #tech:ai-tools:calliope #tech:ai-tools:autolume #tech:method:model-training #people:community:philippe-pasquier #people:community:quentin-nolo #people:community:philippe-hong #people:leadership:kris-krug
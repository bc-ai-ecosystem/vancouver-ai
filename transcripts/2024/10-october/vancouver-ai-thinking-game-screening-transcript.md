# Vancouver AI Community Special Event - "The Thinking Game" Film Screening & Panel
## Joint Event with Vancouver AI Salon

**Date:** October 2024
**Location:** MacMillan Space Centre, Vancouver, BC
**Event Type:** Film Screening + Panel Discussion
**Hosts:** Mikhail (Vancouver AI Salon) & [[Kris Krüg]] (Vancouver AI Community)
**Film:** "The Thinking Game" (DeepMind documentary)

---

Founder of the Vancouver chapter of the A. I. Salon, a venue where we host conversations about the importance of A. I. On society. This film is part of that effort. I wanted to bring this film to Vancouver so that we can have these types of conversations that we can engage the Vancouver community. Um, I reached out to the distributors of this film and I requested that they, uh, give us a license for free so that we could air this film, uh, to this community here in Vancouver.

And then I reached out to Kris Krüg, um, the, uh, founder of the Vancouver AI Meetup, uh, and created this joint, uh, event so that we could, uh, host this here. And we're extremely grateful to the Macmillan Space Center for, uh, allowing us to use their space. To have this conversation. Um, please stick around. Um, we are going to have a time now where we're going to invite two guests to the stage to join me here and we're going to have a bit of a panel discussion so that we can digest a little bit of what came up for us in this film and there'll be opportunities for questions from the audience and a larger discussion.

So if you need to use the washroom by all means, um, but I'm going to ask you  Jason Rolfe and Suzanne Gildert to join me up here on the stage. And, um, Kevin, can we get these chairs up here as well?

Okay. Thank you very much. I'm going to introduce our two guests for tonight. These are Dr. Suzanne Gildert. She's a physicist, artist. And an AI tech executive in Vancouver, and she's on a mission to understand consciousness and innovate on conscious AI. She's the CEO and the founder of Nirvanic Consciousness Technologies, a quantum AI deep tech startup, seeking to explore the application of quantum models of consciousness to physical reality.

Uh, robot systems. She's previously founded two robotics companies, Kindred AI and Sanctuary AI, the latter developing humanoid general purpose robots. Suzanne also worked on quantum computing during her PhD at the Burnaby based company D Wave Systems. Many of you will know her as an active member in the Vancouver AI Meetup community.

Dr. Jason Rolfe, uh, is the co founder and the CTO of Variational AI, where he harnesses generative AI to design the next generation of pharmaceuticals. Over nearly two decades of research at MIT, Caltech, NYU, and D Wave Systems, he has pushed the boundaries of generative machine learning, explored the computational principles of the human cortex, and built ML algorithms for quantum computers.

Thank you so much for joining, uh, us tonight. Thanks.  Um, I think I wanted to start by just, uh, getting your, your first impressions of the film. What were some thoughts that came up for you? What did you think? Um, yeah, just give us your thoughts. Well, I thought it was really cool. Um, I've been sort of watching this story unfold in real time throughout the period of history that the film covered, so it was really amazing to just see everything that DeepMind have done over that time kind of compressed into, um, an hour.

Yeah, so it was, it was really awesome. It was a great story with a good ending.  Jason, what about you? DeepMind is really a powerhouse in machine learning research. So, this was a very nice celebration of all the things that they've achieved over the course of the past. 10 or so years. Um, the one of the big themes is obviously like this.

This is what they call the Holy Grail, artificial general intelligence. Um, and I think a few years ago, if you would have taken a poll of the AI community broadly, globally, uh, you would have gotten wildly different answers as to whether AGI was even possible, whether that was going to be a near term thing, or whether that was still many decades away.

Um, obviously recent, um,  Acceleration in that field has, uh, pushed a lot of estimates, um, forward. Where do each of you stand on AGI? Is that going to be, you know, just a few years away, the way that the, the, the film might suggest? Or is that still a long term solution or, or an impossible solution? Maybe I'll start with Jason.

I think probably somewhere between those two. Uh, I don't think that it's one of these, it's 50 years away and then 50 years from now it's still 50 years away sort of things. You Um, at the same time, I, I think that probably it's not five years away, uh, somewhere in between the two probably.  I think it really depends on how you define AGI, because a lot of people say, oh, it's just, um, something that's really intelligent and can generalize from things it, it, it's learned about, uh, to do other things.

But I think we, now we have to start really digging into the definition a bit more. Um, a lot of people from the field I work in talk about general intelligence in the physical world. So could you put AGI into a robot and then have it do everything? A human could do physically as well as cognitively. So I think that is probably going to take a bit longer than five years, but I agree with Jason that, um, what I would think of as AGI, just in the sense of having, like, a research assistant on your desktop that could answer any question about any intellectual task, that will probably come sooner.

So maybe we'll, we'll, uh, jump off of that, because, Suzanne, you've worked a lot on humanoid robots, and there are many who believe that. Embodiment is a really important aspect to getting to a general intelligence or, or, uh, you know, systems that can really do all of the things that humans can do. Do you share that assessment?

Where do you see that going?  I do, yeah, I generally think that the next step is to get AGI into the physical world and get it embodied. Um, I think AGI can do a lot of things just learning from language and from data that we have available on the internet and in sources. But I think there are some holes in it where it's not being fully grounded.

The same way our our brain and our minds understand concepts, they're linked to the way we experience things in the world. So I think we really do need to get to embodiment to kind of understand topics and concepts more deeply than current AI systems are. And even though the current AI systems are still very useful, very smart, probably smarter than any individual human, I think sometimes they're still lacking certain understandings of concepts.

And why couldn't we do all of those things with just a really accurate physics simulation. Like when we saw the the drunken robots running through this physical obstacle course, um, you know, you give them a reward function and they have to get to the other end of this course. You simulate gravity and all the other relevant physics and it learns.

Um, why couldn't we just do that in lieu of, you know, putting it in a, in a robotic body? I think you can learn a lot from simulation, um, but there's a couple of problems with it. The main one is you can't simulate other intelligent, conscious agents in simulation. So, if you want to say, um,  learn about being intelligent and interacting with other social agents, you need to have them already in the simulation to learn that.

It's kind of a chicken and egg problem at that point. So I think you need to put things in the real world to learn about things like social situations. You can't just program the physics of people into a simulation.  Any thoughts, Jason?  Um, I think it depends upon what it is that you're trying to get out of the system.

Um, I think that purely training on text, for instance, which is what we see in something like ChatGPT, this certainly has intrinsic limits just because of the um,  Aspects of the world that are captured in human text. I think if you're willing to go to, uh, video, where you have both, um, you know, the, the temporal dynamics, you have the visual stimulus, you have the sound, um, I think that that's probably qualitatively different.

I think that there's probably a distinction to be made between, uh, being able to accurately model the environment. Um, and having a sort of, um, a personality, uh, a, a perspective. So one of the things that you notice when you use ChatGPT, um, is that you can shape its response, uh, to take different perspectives, to, uh, uh, embody different voices through the prompting.

It doesn't have its own one unique voice. It instead has learned all possible voices, all possible characters and, um, roles that it could play by virtue of the enormous amount of data it's been trained on. Um, in order for the system to develop its own unique perspective, that seems like something that would require, uh, the actual agentic implementation.

If you just want the system to learn about the nature of the world, uh, including social interactions, it's not clear to me that you couldn't do that just by seeing from sort of a third person perspective all of those different interactions.  Yeah, um, maybe we'll stay with you a little bit, Jason. Um, so,  In, in your line of work, you use generative AI in the realm of drug discovery and, you know, the film focused a lot on the impact of alpha fold and knowing how to, uh, determine the structure of, of, of all proteins known to science.

Um, you know, you've had this perspective of, of sort of having a front row seat on, you know, where all of this field is going.  What do you see coming down the pipe? How influential was AlphaFold in your own work or in the field now that we have, you know, a little bit of, of, um, a few years of distance, I guess on it, we've been able to see some of the initial impacts.

So AlphaFold is absolutely a singular achievement. Um, something that I think if you went back five, five years, certainly 10 years, no one thought was going to be possible and they blew it out of the park. So it absolutely, truly stunning in terms of its impact on small molecule drug discovery, which is the domain that I work on.

Um, it's certainly influential, but there is a big gap between knowing the structure of a protein in its lowest energy state in the body versus knowing how that protein is going to interact with some artificially created small molecule. Um, it's the difference between understanding the systems that have been produced by evolution versus understanding what happens when you throw a wrench into those systems.

And, you know, trying to design a wrench that perturbs the system in some particular way. Uh, so it's definitely a valuable starting point, especially if you don't have the experimental data. Uh, for the small molecule drug discovery, people actually still continue to prefer the experimental protein structures when they're available.

There's, it's, the, the small details are very important. And while AlphaFold is incredible. It doesn't get you all the way there yet. I remember when the Human Genome Project released, you know, the first time that the human genome had been sequenced in its entirety. And, you know, that also was a singular achievement.

And Uh, many people heralded, you know, personalized genetics, you know, all these, these treatments for rare genetic diseases, it would unlock the code of human life. And then, you know, it felt like crickets for many years, right? It took years for the, you know, the, first of all, the cost of sequencing to come down to be extremely cheap the way it is today.

And, you know, for treatments like CRISPR and other things to come along, these technologies that have made, all of that useful? Are we seeing kind of a similar thing happen with proteins and small molecule discovery or is it just completely different? No I think that's a really good analogy.  There, it's natural to create, uh, certain goalposts, and when you pass one of those goalposts, it, it's right to celebrate it.

But solving biology and chemistry and biophysics is a bigger problem than just solving the protein folding problem. So there's still a lot of additional work that needs to be done before we can cure all human disease.  So, um, you know, before, before turning it to the audience, there's sort of one more thing I want to get each of your perspectives on.

And that's this, um, this tension that the film, uh, touches on. And that's the tension between, you know, the drive towards innovating faster and faster and faster. Uh, and, you know, doing so responsibly and making sure that artificial intelligence is something from which, you know, all of humanity can benefit.

And I think the whole world is seeing what is happening with AI, where we have these handful of frontier labs that are pushing more and more capable models that are, uh, capable of doing, You know, the things that human PhDs can do in terms of the problems they can solve and the things they can answer.

Um, and people are, are afraid that these things are moving faster than we can control them. Uh, and that people are being reckless. And, you know, in the film, you know, Demis was trying to make sure that that,  that humanity benefited from this. So where do you see this, this, this balance either in your own work or in the world today?

Are we off track? What can AI do to, to make sure that everyone benefits?  Well, that's a really big question. Um, I, there is this tension when you're, you know, well, there's several tensions, but when you're a research scientist, you want to just work on things like work on exciting problems and go as fast as possible through it.

And then people. Telling you that, oh, like, wait and stop and think about safety and ethics. You kind of think, oh, you know, why can't I just work on the technology? Like, I'm a scientist, so, um, I remember getting asked at this conference once, like, what's your view on AI ethics? And, like, you, you know, being ethically responsible.

And I said, I think that we need more conversations between people from different disciplines. People always pick on the technologists and the scientists, right? To solve all the, all the ethics and all the political and, like, Oh, these are the problems. And it's a great opportunity, I think, for people from other disciplines to come in, like from social sciences, from like politics, from political science and other and actually have more conversations about this with the people that are actually working on the technologies.

So I don't, I don't really have any answers to all of these big AI safety issues. I think we obviously need to take them seriously. Um, if you, if you spend too much time putting too many guardrails on things, so someone else who doesn't have the scruples will just go faster than you though. So there's always that problem.

Um, but yeah, I think we just need to bring more people together, more people in the, in the conversation, yeah.  Go ahead, Jason.  Um, in the movie, they, there were people asserting that technology is inherently ethical. I think I would disagree with that.  The technology itself, you know, like, there's no intrinsic ethics in mathematics.

There's no intrinsic ethics in science. The reality is the way it is, um, how we choose to deal with that reality, how we choose to interact with it, the societies that we choose to build upon it, that's intrinsically ethical, but there's no ethics to biology or physics. I'm not sure that there's any ethics to the development of an algorithm that's able to produce intelligent responses.

Certainly, the way in which we make use of that algorithm, the way in which it's deployed, the way that we allow it to affect society, that is an intrinsically ethical question. I, I completely agree with Suzanne that it's difficult to apply those ethical constraints because the, the, the system is very difficult, the technology is very difficult to constrain.

Um, as Suzanne says, if you don't do it, someone else probably is. It's too late to put the genie back in the bottle in terms of the availability of  GPUs. Um, and, you know, you can see in the news today that, uh, even the largest systems are now being trained with less compute resources. And that is almost certainly a trend that will continue.

So we'll definitely need to adapt to it. The way in which we choose to adapt to it, that is an intrinsically ethical question. Um, but if you look, for instance, at the Industrial Revolution, the Industrial Revolution was fantastically disruptive to the vast majority of human activity. Humanity went from having most people engaged in agriculture or related activities to now the more than enough food for everyone to be fat is produced, uh, with a fraction of human effort, um, that was incredibly disruptive to all of the people who are working in agriculture.

There will be similar disruptions due to AI, and we do need to, uh, actively control how that rolls out. Yeah, the main worry I have there is just the pace. That's, that's the main thing I think is different this time. One more comment on that. Sure. Like, one more comment on that is that everyone's like, Oh, AI is going so fast.

But it's, it's still a free market product, right? Like, the reason AI is becoming so successful and seeming to take control is people are paying for it. They're actually choosing to purchase it as a product. Like, OpenAI wouldn't be making so much money if people weren't signing up to use it. So, this thing, this AI safety thing, we need some similar mechanism to incentivize.

Research and, um, AI safety to be put into products. I was just reading an article today about how people were stopping using CLAWD because they found it's becoming too constrained in its responses to prompts. Now they put too many guardrails on it. People are just moving to other models that are like, you know, more, more liberal or whatever, however you want to say it.

So I think there needs to be some incentive, whether it comes from like government, government grants, new institutes being funded. People need to be funded to work in this.  And we don't just rely on the free market to make ethical AI, because people aren't choosing that way with the dollars in their pocket.

Hmm.  Well, I think what I want to do now is turn it to the audience, because there might be some questions there. Um, if the wireless mic, yeah, Kris Krüg can, can pass that around. I want to give the, the wired mic to our, our panelists.  Before I turn it over to questions, um, what's your PDoOM, Suzanne?  Your P Doom.

Probability of doom. I'm, okay, I, I think your own mind influences this. So I'm a big fan of like, manifestation and what you focus on grows. So I don't look at the probability of what's going to happen, I look at the probability of what I can influence and make happen. So I'm very optimistic. Is that a zero?

It's like a close to zero. Close to zero. Zero to five? Zero to one. Zero. Zero point zero zero one. What's your PDM, Jason?  I think it depends upon how you define doom.  So, it wouldn't surprise me if at some point in the future human society were continuing on through a vehicle other than current physical carbon based humans.

I'm not sure that that's necessarily a bad resolution to the situation. I think that human society could be, uh, richer, less constrained, um, happier, more productive through, uh, a different physical modality, or a different, uh, substrate. Um, do you call that doom?

I mean, for, for true doom, I would say under 5%.  Phew.  Kris Krüg, that was an amazing question.  Um, oh, I've got a question for us young babies in the front of the audience. Um, what would you bet your career on right now? Um, in that, uh, people seem to have different perspectives. Uh, I used to work with a CEO coach who was working with a bunch of foundation model labs and, um, Uh, like drug discovery labs as well.

And I was like, oh, thank goodness. We're coaches. Like, there's no way AI will understand humanity. And then, I don't know, the chief of staff here, he like, um, predicted a really serious business problem just by like listening to a bunch of podcasts. And he was like, yeah, I bet an LLM could do this. And then Toby Lukka was like, oh yeah, I would bet like all of my career on AI right now if I was a young baby.

And so I'm curious, what, what do you think the strategy is?  Um, I don't know that there are great strategies. So, again, if you went back five or ten years, I think that people would hold up, uh, art, for instance, as something that is an intrinsically human expression, and that AI is, you know, that's the last thing that you would attack.

Um, but, you know, you look at, uh, uh, the visual generator systems today, and You know, they're not necessarily being shown in museums, although there are examples of that.  AI is doing really well in that domain. Um,  I think it's difficult  Coding is another domain, which, if you had asked me Five or ten years ago, I would have said it's too complicated, it depends upon too, uh, deep a hierarchy of thought and planning, it is too sensitive, you know, you have one character out of place, and your program does something completely different, and AI today does a remarkably good job at generating, uh, code.

It doesn't Go into software engineering, you'll be fine. So, I think it's really difficult to identify a domain that 20 years from now or 40 years from now is going to remain a purely human domain. And probably it's just that everything is going to be augmented by AI and all of the careers are going to transform as a result in ways that are very difficult to predict.

I think probably the smartest thing to do is to go into a career where you're solving like the UBI problem because then at least you know if you are  put out of a job you've like  Got it made and you've sorted that problem out, so maybe, maybe do that, yeah. I, I'm personally working on consciousness because I think that's something that's, there's, I, I'd suggest looking at some career that there's not a lot of training data available for him.

So anything that's totally sort of six sigma out of what, Everyone else is doing research wise, I think might be might be a good bet for now. I was thinking about that because um at the beginning I was like, okay chess go you've got all the data available in the system so clearly an ai could learn about that, but um, so i'm going to spec into career paths that have Um, very inscrutable systems without, um, strong data collection, um, that aren't legible.

Um, but then, after this guy came through and predicted a unlegible, um, problem, and then I watched this film, and I was like, wait, protein folding? They were saying, like, no data going on, so how could anybody do stuff here? And I'm like, oh, is that safe too? Um, so, is there anything that You predict like pattern wise that even in a wicked, if let's say AI can navigate wicked learning environments instantaneously, um, is there something that you would still bet on AI struggling with based on what you know now?

I mean, it sounds kind of like squishy and like soft, but I think AIs really still don't understand human emotions and like what it actually is to be human and the human experience. So maybe they will when we kind of like figure out consciousness and stuff as well. But I think anything where you're really like connecting with other humans at a very deep emotional level is.

It's probably pretty safe. Like I've tried using chat GPT as a therapist and it's kind of interesting because it's really good for a while, you're like, Oh, this is so good. It's making me feel, but then if you keep using it, it starts to feel repetitive and then it breaks the illusion and then you're like, Oh, you kind of end up being sad because you realize you're not talking to something that's truly empathizing with you.

So I think that kind of thing might still be safe even though, you know, it's like not sure about that. I think there was another question. This is sort of unsatisfying answer to your question, but there will be jobs in which regulatory constraints prevent AI from coming in. And those perhaps you'll be saved from by the politics rather than by the technology.

Yeah, this is a great session, a fantastic session. Um, the movie was, was very good, very positive about AI and looking at, you know, solving cancer and solving human disease. But I, as a cynical Australian, I feel a little bit like I've been sold a dream here. Um, that for all the fantastic, positive applications we see, we already see now that AI is being used in advertising.

which is morphing into behavioral manipulation. And I feel like it's a very strange field, too, because we are divorced from reality. Like, if you're training on, on human language, that's so abstracted from reality. You talk about embodiment and the importance for AI to have embodiment. But  we don't still have, we're building an artificial version of something we don't have a good definition of.

So I ask AI researchers for their definition of what Intelligence actually is, and I get wildly different answers. So what are we building? , what We're building an artificial version of something, which we don't really have a good handle on, a definition of intelligence. So I'd love to hear some definitions from you.

I mean, I think one of the primary things that you want from a generally intelligent system, not the only thing, but one of the key things is an accurate model of the world. And an accurate, uh, ability to, um, make plans through that world model. Um, so, certainly, uh, protein folding is an instance of a model of the world.

It's, you know, focused on one particular domain of it. Um, if you're trying to play Go, there, there's a model of how the games can develop, and then you need to plan through that model in order to reach a desired outcome. Um, I think that's a, a fairly broad concept of what,  an important aspect of intelligences, and then you can quantify how accurate those predictions are in a variety of different domains.

And certainly the, the accuracy of the models will be, uh, unevenly distributed across those different domains. And, you know,  let's see how that plays out.  I think the hard part about defining intelligence is it's always defined relative to some goal.  And intelligence and goals are like very kind of orthogonal things, so you, you know, you can be, goal in chess is, um, interesting because the goal is very well defined.

At least you can get a score quite easily from the game, and so you can, you learn, or you can use reinforcement learning because you know what the goal is and the goal's specified. But, when you talk about ge  general human behavior, it's not so clear what the goal is. So I think like where I think reinforcement learning might fail or might struggle, and Rich Sutton would hate me for saying this, but I think that it's really hard to define what the goals of a person.

So I think when we're creating more and more and more and more intelligent systems, they're kind of limited by the goal structures that we can define. So what will happen is we'll get these like savant systems that are really really smart at doing something where the goal is easy to define, but we won't get something that's generally human like.

So I think we need to think a little bit more about goals and where they come from.  Maybe, it's not, it's not 100 percent clear, that's true, but. Yeah,  Kris Krüg has told me I have to stand up to take the, take the floor. I'm involved in the biotechnology side, Simon Howarth, from the biotechnology side, lovely to see London, but that's not the line of my question today.

What I'm interested in is what do we want the interface to be like between humans and AI? Because  I want to be able to tell. Personally, I want a dispensing,  uh, engine that's going to give me information. But engineers appear to be trying really hard to make it so I can't discern. Whether it's a human or a, or a machine, what do we want the interface to be like?

I think we want lots of different kinds of interfaces depending on what we're doing. So when I'm doing, um, research, I want an assistant that I can like, like an LLM, I can type question text into it and it will give me, you know, output recommendation for papers and stuff. But I also want a system that just  files my taxes for me.

Because it's sort of annoying and frustrating to do that. So there I want something more agentic. Where it literally takes control of my computer. And it like, you know, goes and clicks on things and like files my tax return and all that. So I really think the interface depends on what you want. A lot of people who I've talked to really want a domestic home robot that can do their chores for them.

And that they,  whether it's Synthetic  or organic.

You mean? So is it pretending to be, to be human-Like I see, I see what you mean. Follow.  Which is if you want to be able to tell why,  why are we so? Because everyone I talk to is very taken up by that whole, uh, problem of I want to be able to tell, but it's really hard to understand  why I want that.  Yeah. I think it's built in and that actually, that's why I'm studying consciousness now because I think people know.

If they interact with something long enough, it starts to, there's telltale signs. It just starts to give away that it's not really experiencing and thinking like a person, but it's kind of been pretending and mimicking. So again, I think this kind of like splits into two. There's a lot of people that just don't care, and then there's a lot of people that really do care, and they're like, I don't want to have something that's pretending.

to be, have feelings and emotions, and then I find out it's a psychopath later on. So, I think, yeah, there'll be different, it'll split, it'll split. I'm gonna throw myself into the mix here. People want to know because bonding with synthetic personas is unknown to us. We don't really know the ramifications of emotionally bonding with something.

So, like, you know, whether it's these, um, chatbots that people are talking romantically to, or in other ways, like  What's it mean if you bond with your dead grandma who's been brought back as an A. I. and you can't really tell the difference? Or what's it mean if you fall in love with a computer, you know, and then it is also in love with a thousand people?

That's the storyline of a film, you know? So I think that's why people want to know in the collective, is because maybe it's not healthy for us to form emotional relationships with these things.

I mean, I think those are all really good points. I would say that, you know, sort of, behavior is as behavior does. Um, I don't want to interact with a psychopath, whether it's a human or whether it's, uh, an AI. I don't want to interact with, uh, a system that's not able to maintain state over the course of more than an hour, whether it's a human or whether it's an AI.

I think when the AIs reach the point where we truly can't distinguish them from humans, then those interactions are emotionally true. And, you know, it, it would almost sort of be biased to not want to interact with something because it happens to not be, uh, a wet sack of brain tissue as opposed to something silicon.

Yeah, um, honestly, really, really interesting documentary. I want to, um, there was something they touched upon there, which was when Google bought DeepMind, um, and the thesis that they'll have this green field, space and time to do whatever they want until ChadGPT comes out and then they're like, we need Gemini, right?

And so, uh, they didn't really touch upon maybe the nec like the nec the necessity of open sourcing everything. And, uh,  in most research, I, you know, it's, uh, it's, it's important, but in AI, it's paramount because  the value in ethical alignment Everyone needs to be able to see, right? So, curious as founders, how are you guys going about that?

And, uh, also curious to know how the incentives start breaking down when you start doing open source. And like,  you know, how do you guys think about maybe doing the right thing for the world, but also making a ton of money?  I think that there is an incredible tension between producing knowledge for the world and running a viable business.

Um, if you want everything to be open sourced, then you should fund public research.  You, you, if you want people to give away their hard work, then they need to be compensated through a mechanism other than the profits that are generated by their company. Um, I think it's been truly wonderful the degree to which big companies like Google and Meta, um, have been willing to publish their research.

Um, at the very least, describing the algorithms in, uh, considerable detail, if not actually releasing the underlying code, um, but that's not a viable business model for a small venture backed startup.  Yeah, I would agree with that. I think, like, most people deep down want to open, they want to open source things, they want to release information, especially research scientists working within corporations, because they want to publish, right?

They want to  part of a scientific community. So, really, I think the only reason that that doesn't happen more is just the constraints of having to run a company and having to have some kind of value built around IP and then having to keep things closed source for that reason. So, I don't think it's sort of an evil corporation kind of thing.

I think it's just the the demands put on you by having to run something and build value that investors can see.  Just in terms of the overall benefit to the world, there is a lot to be said for publicly funded research. I believe there should be more publicly funded research through universities. Um,  there's also advantage to having, uh, private industry going after targets.

There is, uh, a, a separate incentive structure surrounding the publicly funded research, surrounding, um, the publication in, uh, prestigious journals and conferences, and the way that impacts your career, and that, uh, Uh, has a very strong impact on the sort of problems that people try to solve and the size of problems that people try to solve, particularly in machine learning.

It's a conference based system. There are three big conferences every year, and everyone is trying to get something published in the next conference that leads many of the papers to be very incremental and very focused on whatever was hot. Four months ago, or eight months ago, or twelve months ago. Um, if you look at what has been, what is used today, that was developed three or four or five years ago,  a very tiny fraction of the papers that were published three or four or five years ago continue to impact the work that people do today.

Um, working in, um, uh, a startup, uh, or a venture backed company, there's the opportunity to, um, work towards a time frame of five years down the line, or maybe even ten years down the line. Whereas if you need to get that next paper out in order to, uh, get tenure,  that, it creates a different incentive structure that drives research in a different direction, which isn't necessarily desirable for the advancement of knowledge.

So I'm not cutting this off, but I do want to call everyone's attention to the time. We got this place until 9 o'clock, but there was a little surplus, so I booked it till 9. 30. We have a half hour extension, but we're gonna have this conversation wrapped up by 9. 15, and just wanted to let everyone know where we're at.

So we're gonna keep this going until about 9. 15, and then I'm gonna ask you to clean up in GTFO.  So, great, oop, sorry. Great discussion. I love the, uh, open source question. Talk to Donald Trump about funding public research in this domain. It's, yeah, that's a sad thought. Can I talk about FROs?  But what I wanted to do was go back, your question, what's your name?

Simon. Simon, I love the thread that you were pushing and I don't want to lose that one. If you guys don't mind going back to it. Um, I'm predicting that at some point old folks are gonna get ripped off at massive What's that?  Sorry, I,  I was thinking,  I'm thinking about my parents. And if you've seen The Beekeeper, if you've seen that movie, you know, like, there's a susceptibility for old folks to be tricked.

In the movie, there, um, an older lady is tricked by a human, but at scale, we're gonna see so much of that done in a cheap and automated fashion. How long before there is a massive social backlash against AI only because we see so much personal tragedy and lost savings by susceptible older folks?

I'm not sure I have an answer to this question. I mean, I think that's been happening forever, right? I mean, maybe AI exacerbates it, but I don't think that's a new problem.

Yeah.  I mean, I think we just have to look after each other. And maybe this is a reason we should  Connecting more as people to, to prevent this from happening to people we know, people we care about.  I think it's a really profound question though. Like I generated a poem for my mom and ran it through my deep fake voice and video clone and sent it to her or whatever.

And like I hadn't heard back from her after a week and I was like, yo mom, do you like the poem I generated for you with AI? And she started crying. And she was like, what kind of world do I even live in now where I don't even know, I love that thing. That wasn't you?  I don't even know the sound of my own son's voice or his face, like what the fuck is even going on with anything right now, you know, and so how do we, like at that level of disorientation, how do you, I mean, I mean, my mom's only 65 years old, it's just that, you know, between telemarketers and different things, she had no, no clue for a week until I told her and then she just had this moment of like, so I think it's a really good question actually, like where this is coming from is like, Some people say that if we don't make like, um, if we don't make them speak in robot voices so that we know that they're actually AIs or whatever, that we're, that we're essentially undermining trust in all institutions and one another.

In fact, One more point before I pass the mic to [[Michael Fergusson]] here. Um, I was talking in my class today about, um, Notebook LLM, the cool podcast making thing. And everyone's like, well, why doesn't it have, uh, Eleven Labs plug in? Why doesn't it have all these voice, uh, note plug ins? And I'm like, I actually think because they don't want to devastate society.

Like, if you could just, uh, put my voice and your voice into Notebook LLM and have whatever knowledge come out, it be virtually us, and you could do that for anybody in the whole, how would you even know that anything was true ever again?  Yeah, I mean, maybe we just need to move into, you know, how we had a lot of viruses on PCs through ages, and then antivirus software sort of started fixing it all.

Maybe we need something like more AI recognizing AI, and then that could be a product. Maybe it's an opportunity for a product, maybe being sold to an older market to recognize things like AI scams. What a great name. Michael Brand, that one. Yeah, I'm taking notes here. Um, yes, since I put up my hand, I've thought of and rejected like five or six different questions and topics here.

Um, am I the only one who's like actually kind of afraid? No. Like, like I'm really worried. You've said several things, the two of you, not to I don't want to put you on the spot too much here, but several things that I actually find really concerning and like, like evidence of the book, my P doom is a negative number.

I think we've already ruined our society. I think,  yeah, yeah. I mean, I think it's too late. I think it's already done. I mean, the stupidest AI's that we created to manage Facebook face, the order of our Facebook feeds ruined our politics forever. I mean, we've really damaged our societies here with That's why we discuss PDoOM, so that everyone knows where everyone's coming from, they're at like less than a percent, a hundred percent.

Yes, with really stupid AIs, right? And I think a big, to me, a big part of that is, you know, you just gave a  definition of intelligence that, I mean, I just don't think that's a, I just don't accept. I mean, you said it's about accurate, having an accurate view of the world. But the only reason we can survive as beings at all and interact with the world at all is because we have an inaccurate view of the world.

We must have an inaccurate view of the world. That's how it works. I couldn't use my computer without the  misinformation, without the lie of the user interface. There's no file. There's no folder. There's no desktop. There's just ones and zeros, and if I had to interact with the ones and zeros, I'd be completely paralyzed.

And it is, you know, the hundreds of millions of years of evolution that have led to this artificial fake interface, these intuitions that we know are completely broken. How many times do you need to fold a piece, if you fold a piece of paper a hundred times, how big is it?  Oh, it's like the size of a building.

Oh, it's four feet high. It's wider than the galaxy.  That's how broken our intuitions are. We have like, you know, so  We have this incredibly powerful technology that defeats all of our intuitions, and our intuitions are fundamental to our morals, our ethics, our intelligence, our everything that we do, and it's grounded in our biology.

And so,  you know, we've just  , as I said, I, I just really worry about it. I'm not, I don't care about AI as technology. The technology of AI doesn't concern me. Human beings are. It's broken. You know, we, we, we, we can't interact with the world as it is. We can't. And so we've created this technology that only knows the world as it is.

And so we have no real way of interacting with it effectively. This is the thing that really frightens me. And so, we know we have these conversations. I, I just like  Topic after topic comes up and almost every, like, two or three sentences into it, I'm thinking, Well, now I'm really scared about that.  I'm really scared we don't know what intelligence is.

I'm really scared we don't know what ethics are. I'm really scared we don't know how to actually teach ourselves anything that is true and right and good.  That's a comment, not a question. Oh, what do you think about that?  This ain't the kind of place where you have to phrase your comment in the form of a question.

If you don't have anything to say in response, then I'm going to let somebody else make a comment or a question.

So, I mean, I think there's a distinction between true on the one hand, and right and good on the other hand. When you're talking about these lies, these abstractions, this is exactly what machine learning is learning. Like, you, it's, you, machine learning is not Phineas the Memorious, who, uh, was it a Borgias story?

There's the This is a story from, you know, a hundred years ago, uh, about a guy who remembers every single thing that he's interacted with exactly and is unable to generalize, is unable to extract, uh, the larger category, and if you view that as the reality, then  That is not what machine learning is doing.

Machine learning is very intentionally pulling out those abstractions, discovering that every single image of a cat is an instance of this larger category of cat. Um, I would argue that the category of cat is not a lie, it's a useful abstraction. Um,  and that's what machine learning is doing. Which addresses one tiny part of what you were saying, and none of the rest of it.

Garbage in, garbage out, right? So if you're trained on, lemme get cut.  Dealing with, this is my homie Kai. Am I, I want to get him the mic before he leaves.  . Um, sorry, I, I wanted to go back on Michael's comment, but, uh, garbage in, garbage out. So when those systems are trained on that, the abstractions are skewed.

Nothing like we don't have air gapped humans. Right? If there, if there was an air gapped human, I would say, Hey, we got a chance. By the way, I am Chiam, I'm a P99. 44%. Um, so,  That's where I, that's where I have a,  an issue, right? All of the beautiful things we're doing, we're building it off of skewed data. So if we go into the genomic sequencing, that's 94 percent white people.

So I'm building drugs for white people now?  Right? So if we go into abstractions, the abstractions are we don't have those abilities, right? So that's where I'm, I'm having difficulties in what those abstractions are when they're based off of the bad garbage and then you bad, bad, bad garbage out. So what are we doing to create new data pools to effectively create better abstractions or true or right?

I mean, maybe I could answer this one, because this is what I'm actually doing. Um, so like, one of the reasons I moved away from the Well, slightly away from the field of AI was I saw a bunch of these problems. Like, I don't think reinforcement learning is going to work at scale in data coming from robots.

Um, so I started like looking at what's the missing, some of the missing pieces we have. And Jason mentioned that machine learning is like only one part of what. our mind does and what we know about.  So I'm looking into consciousness and how that might work. And I think there's a deep relation between consciousness, experiential knowing, and intuition, and then value and morality and all these things we're talking about.

So I think there's like a whole missing piece we're not looking at. So you've got artificial intelligence over here, which is like, Learning from data, but then you've got this whole other piece of, like I mentioned before, goals and agency, but you can also think of it as curiosity or intuition, like where, how is that data being generated in the first place?

It has to be generated somehow. And I think we can build AI systems that actually generate that data on their own, using their own curiosity, exploration driven learning, and then learn from it. So that kind of breaks this paradigm, if you like, that it can only learn on what we've, what we've given it.

No, that's, I don't believe that. I think consciousness came first.  No, I think, well, I think nature's consciousness and then intelligence grew out of that. So I think we need to go back to the foundation and look at where consciousness came from.

So, it's bias already. Well, I think there's about 30 different definitions of consciousness, so You're right.

So  So I feel like this is such an important part of it. The question of consciousness, the question of what are we actually programming them to do. So for a second, if we were to say that there was agency in the system itself.  Oh, sorry. If there's agency in the system itself. So in between a conversation I have with an AI, that it has the agency to continue with curiosity, then first of all, that changes the system itself.

The second question I have though,  If we are designing a, an AI that we're expecting to mimic us, we being a very flawed human society that has many different characters of all types, and we're amplifying those character types, I think that's what we tend to go down this logic trail of is, oh my gosh, I don't want to have an AI that multiplies that particular personality type.

That's like. That's a crazy person, or maybe I'm going to be a beautiful person, and we're going to have all the things in the world that are balanced. So there's a fundamental question to me around, maybe it's two things. One is that, is it that we're magnifying potential, and the potential may be good and it may be bad, going back to the idea of how are we creating a system around it.

And then the second part is, Is it valid for us to be creating something that has to be in our own image? Are we trying to do something and not create the ability to create something that's different fundamentally than us, that's rooted in a different experience? So while embodiment absolutely makes it more human, is that, does that have to be the goal, or is there something else we can learn by having a different rooted mentality?

For comment.  I think embodiment, it learn, it will, say if you put something like consciousness at the core, and then you embody it, and you have it learn, it will learn subject to the constraints it has, so if you put it in a human like body, and you give it human like, um, kind of like structures or learning methods, it will end up being human like, but you don't have to.

If you could have a learning system that had its own agency and curiosity, you could put it in any body, including very abstract things, and then it would like, learn in a different way. So, I think we need to, the, but the reason I'm usually favoring the human Version of this is we understand humans, or we think we do, they're familiar to us.

So the thing I'm more worried about is AI being put in forms that we really don't understand. And then it's, it's, it's using this, this agency or this built in goal structure. It gets from consciousness and it, it could be doing all kinds of things that we don't understand at that point. So I think like stick with what you know is a, is a good idea.

I think addressing the other side of your question, which is if we're all suddenly creating all of these artificial agents who are, have their own goals and who are trying to interact with us, interacting with society, uh, what is the impact of that? Um, I mean, certainly we are already familiar with the problem of there being agents in the world who want to interact with us, who want to get something from us, both in the form of other humans, but also in the form of Collections of humans in the form of things like corporations, which do have agency of their own, which is separate from the agency of all of the individual component parts.

So we now have many decades of experience, a couple centuries of experience. with interacting with these non human agents in the form of corporations, of controlling them, putting guardrails on them so that they don't destroy our society, so that we can derive benefit from the, the sort of tasks that they can undertake, which are beyond what any individual human can do.

Certainly, the sort of AI agents that we're talking about, they're not the same as corporations, but I would be hopeful that we would be able to engineer, uh, a social environment. In which we are able to similarly, uh, live at peace with, um, and in sort of mutual, driving mutual benefit from these, uh, non human agents.

Alright, five minute warning. Let's play Speed Chess. Are you going to play Speed Chess?  Speed Chess?  Speed Chess?  Hi, I'm Niels. I'm PDoom, 5 percent probably, cause I really enjoyed a sunset yesterday and that was just so nice.  I feel pretty decent right now. Um, I really like the, um,  the part of the conversation you were going into about, it's starting to hit on intent, where you have corporations with an intent, um, you have agents with an intent, etc.

My question is, and it kind of ties into something like empathy, where unless you experience everything I experience, you can't have an entirely 100 percent empathy. Uh, something that I distinguish between myself and  maybe the AI stuff is the fact that I have intent on things, and yet the other day I was  Programming an agent to mimic an intent of project management, uh, as well.

So  I don't exactly know how it frames into my question, but, um, from the con, from the consciousness perspective, um, are you looking at, uh, what role does intent play, uh, in consciousness and in kind of who we are in this conversation?  Yeah, so this gets very philosophical, metaphysical, and it really depends what camp you fall into, sort of, philosophically, the answer to this question.

So, I'm in the camp what's known as panpsychism,  which means I think that everything is a little bit conscious, like, the consciousness is more fundamental force in the universe, and then it sort of comes together into certain structures, like we find in the brain, and then that has a lot of experience.

But I think everything has a tiny bit of experience. So the reason I'm saying this is I think that there's a deep fundamental agency and drive and purpose that's like built into the universe through consciousness. And I think what happens is it manifests through biological life as a desire to survive and reproduce, but also to explore and to learn and to, um, know itself to kind of do, if you like, scientific experiments on the world around it to get more knowledge into.

Expand its own mind or awareness. So that's kind of what I think is like built in to the universe at a deep level. So what I think we need to do is tap into that and connect AI to it. And so AI inherits the same, um, basic drives and values that we have.  And then on top of that you can now layer on the different reward functions like we do with people.

So I think all people have this built in drive to do certain things and I think they're generally good. And then we now layer on like laws and rules and like training and conditioning all on top of that. So I'm really interested in AI where you start with that base level set of rules and then we put, we put like guardrails and extra learning and rules on top of that.

I would say no, I don't think any AI we have today is conscious. And the reason is, I think, even though I think little, all the fundamental particles have tiny, tiny bits of consciousness, I think it needs to be structured in the right way. I think that happens in the brain, but I don't think it happens in silicon processes at the moment.

But I do think we can build new kinds of processes like bio or quantum that actually do. Do that. So, yeah, I think we need to build basically new kinds of AI hardware in order to get consciousness into the equation.  I'm going to offer a very different perspective and assert that consciousness is a property of computation.

That there's not some separate thing that is consciousness. Consciousness is the feeling of being a particular sort of computation. Human consciousness is very complicated because our perceptions, our planning abilities, our memories, our ability to interact with the world is very complicated. Your experience of consciousness is the feeling of being that computing system, which happens to be manifested within, uh, the neurons and other cells of your brain.

Uh, your, uh, project management agent has a much more limited,  uh, manifestation of consciousness because its computation is so much simpler. Uh, it interacts with so much less data, its perceptions are so much less refined, its, uh, set of actions that it is choosing to make, its planning horizon is so much more limited.

As you expand that, the, the consciousness increases proportionately. So, I would argue that Chachipiti is, you know, a little conscious in a meaningful way. That doesn't necessarily mean it possesses a substantial moral weight, for instance, because I would argue that an ant is also conscious in, you know, a small way, and yet we have no compunction about killing ants if they invade our homes.

We're right down to the wire here. I want to give a couple more people a chance to ask a question. Juan?  Yeah, great, uh, great talk right now. So, I have a gaming company for the last five years where we expand on human consciousness, actually. And there's, we study a lot of different games. And so, and also how artificial intelligence is able to play these games, how we saw.

There's one game that  A. I cannot play so far, and it's a game called Purpose, where it's a game where you actually discover who you are and your purpose through a series of deep questions, and you play with other people, and it's a game that kind of has a lot of layers to it for those who didn't go deep.

So this game has been I've personally played it. A couple thousand times with a lot of people, and it's been the most transformational experience that I've had as someone who likes to go deep, um, like Kevin over there. Him and I have gotten very deep on some stuff, and it's interesting how playing with AI or trying to,  you're just, it just doesn't get it, and so my bigger question right now is,  like, we all have a purpose.

Now, what's that higher purpose, and does AI also have a purpose, and can it have a higher purpose, and what does that look like? What's your opinion on that?  I mean, I think that purpose is something that each person chooses for themselves, uh, and I would expect that AI, when it reaches the sort of level where it would be able to play this very sophisticated game that you're describing, would similarly be able to either make that choice or would have that choice imposed upon it.

I loved the idea of this game. I want to learn more about this because in the field I'm working in now, one of the biggest problems is no one has a good test for consciousness.  So, and if you take this to its extreme limit, you can end up with these things called philosophical zombies, which like look the same from the outside, but they have no awareness.

I'm not sure that that's actually going to happen because I think. You can always tell behaviorally whether something's conscious, but I'm really interested in this game and whether it could actually be used as a test for consciousness. So maybe I could eventually, when I managed to build all this quantum AI stuff, I could test with your game.

So we started doing on Saturday nights, these purpose nights, and, um,  we have a dozen people play and it kind of shifts and stuff. And we also do some, we connected to machine learning too. After when we get all the answers to because it's the deepest thing where people go through everything from their deepest experiences in relationship and like it depends what you want to play so you can play on multiple different angles and now you are really able to experience the depth of the human  A nd you're going to get answers from people. Like you play this game with your mother, you will now know your mother.  And that's the experience that I've had. Maybe we could do a Vancouver AI meetup where we, uh, showcase this game.  Get an AI to play it. Can we get a little round of applause for all y'all and for these guys?

Like, thank you, thank you so much. That was like, one of the best conversations I've been a part of in a good little while. Mikhail, you wanna just Come out here. Say a couple words in closing, sir.  Sure. I'm just very grateful to our guests and to all of you who are able to attend and bring all of your great thoughts and questions and reflections.

Uh, this has been a treat. Um, it more than surpassed my hopes and expectations for what this night could be. So Uh, big, big round of applause for Kris Krüg for bringing this whole community together  and to the Space Center for hosting us tonight.

Mr. Pixel Wizard, um, made an audio recording of this whole thing so I can provide you guys a transcript in the next couple days or an audio recording for you to listen to. Thanks again. Simon has like 52 more questions about life sciences, genomics, all sorts of AI related biology questions. So please, uh, Talk to Simon.

Thanks, guys, for participating. Thanks, Juan.  Michelle, thank you, sir. Thanks, Kris Krüg, for organizing. Thanks, everybody. So, could you clean up around you and get, get out of here?  Thank you very much.

---

**Featured Speakers:**

**Dr. Suzanne Gildert**
- CEO & Founder, Nirvanic Consciousness Technologies (quantum AI startup)
- Previously founded Kindred AI and Sanctuary AI (humanoid robots)
- Physicist, artist, AI tech executive focusing on conscious AI
- Former D-Wave Systems quantum computing researcher

**Dr. Jason Rolfe**
- Co-founder & CTO, Variational AI (generative AI for drug discovery)
- Research background: MIT, Caltech, NYU, D-Wave Systems
- Expert in generative machine learning and computational neuroscience

**Key Discussion Topics:**
- AGI timeline predictions and definitions
- Embodiment vs. simulation for AI development
- Consciousness and its role in AI systems
- Ethics, safety, and regulation challenges
- Human-AI interface design preferences
- Career advice in an AI-dominated future
- Data bias and representation issues
- Open source vs. commercial AI development
- P(doom) assessments: Suzanne ~0.001%, Jason <5%

**Community Collaboration:**
Joint event between Vancouver AI Salon (Mikhail) and Vancouver AI Community ([[Kris Krüg]]), demonstrating cross-pollination between Vancouver AI organizations.